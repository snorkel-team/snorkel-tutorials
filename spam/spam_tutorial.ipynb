{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Snorkel Tutorial: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nice introductory text\n",
    "* Purpose of this tutorial...\n",
    "* Steps:\n",
    "    1. Load data\n",
    "    2. Write labeling functions (LFs)\n",
    "    3. Combine with Label Model\n",
    "    4. Predict with Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here's what we're trying to do\n",
    "* Here's where the data came from (cite properly)\n",
    "* Show sample T and F in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits in Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4 splits: train, dev, valid, test\n",
    "* train is large and unlabeled\n",
    "* valid/test is labeled and you don't look at it\n",
    "* best to come up with LFs while looking at data. Options:\n",
    "    * look at train for ideas; no labels, but np.\n",
    "    * label small subset of train (e.g., 100), call it \"dev\"\n",
    "    * in a pinch, use valid set as dev (note though that valid will no longer be good rep of test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading our data. \n",
    "The `load_spam_dataset()` method downloads the raw csv files from the internet, divides them into splits, converts them into dataframes, and shuffles them.\n",
    "As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a particular timeframe in 2014 and 2015.\n",
    "* The first four videos' comments are combined to form the `train` set. This set has no gold labels.\n",
    "* The `dev` set is a random sample of 200 `DataPoints` from the `train` set with gold labels added.\n",
    "* The fifth video is split 50/50 between a validation set (`valid`) and `test` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, df_dev, df_valid, df_test = load_spam_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution varies slightly from class to class, but all are approximately class-balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN  52.4% SPAM\n",
      "DEV    54.0% SPAM\n",
      "VALID  47.0% SPAM\n",
      "TEST   47.0% SPAM\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# For clarity, we'll define constants to represent the class labels for spam, ham, and abstaining.\n",
    "ABSTAIN = 0\n",
    "SPAM = 1\n",
    "HAM = 2\n",
    "\n",
    "for split_name, df in [(\"train\", df_train), (\"dev\", df_dev), (\"valid\", df_valid), (\"test\", df_test)]:\n",
    "    counts = Counter(df[\"LABEL\"].values)\n",
    "    print(f\"{split_name.upper():<6} {counts[SPAM] * 100/sum(counts.values()):0.1f}% SPAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a peek at our data, we see that for each `DataPoint`, we have the following fields:\n",
    "* `COMMENT_ID`: A unique identifier \n",
    "* `AUTHOR`: The user who made the comment\n",
    "* `DATE`: The date the comment was made\n",
    "* `CONTENT`: The comment text\n",
    "* `LABEL`: \n",
    "    * 0 = UNKNOWN/ABSTAIN\n",
    "    * 1 = SPAM\n",
    "    * 2 = HAM (not spam)\n",
    "* `VIDEO_ID`: Which of the five videos in the dataset the comment came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>VIDEO_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>z13kfpwrhzeuvfavb221yxoj3ynsi5vjo04</td>\n",
       "      <td>Warcorpse666</td>\n",
       "      <td>2015-05-26T02:23:11.364000</td>\n",
       "      <td>so spousal abusue cool that&amp;#39;s great﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>z13tczjy5xj0vjmu5231unho1ofey5zdk</td>\n",
       "      <td>LaS Music</td>\n",
       "      <td>2015-05-28T19:23:35.355000</td>\n",
       "      <td>Hey guys, I&amp;#39;m a human.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;But I don&amp;#39;t want to be a human, I want to be a sexy fucking giraffe.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Im fucking with you, I make music, check out my first song! &lt;a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\"&gt;#giraffebruuh&lt;/a&gt;﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>z12rhdkrvvy3hd42c23whdeavkjdxnimg</td>\n",
       "      <td>viviane trinh</td>\n",
       "      <td>2015-05-21T22:35:35.753000</td>\n",
       "      <td>i like the lyrics but not to music video﻿</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>z12svt0reqvqyppzw04cgt1axrqhddcbkfc</td>\n",
       "      <td>derrick lawson</td>\n",
       "      <td>2014-11-12T20:21:27</td>\n",
       "      <td>https://www.facebook.com/FUDAIRYQUEEN?pnref=story﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>z12wuz2qqnawe50js04cejpzosrzdr0r1k40k</td>\n",
       "      <td>Dana Matich</td>\n",
       "      <td>2014-11-08T03:32:55</td>\n",
       "      <td>Hey guys! Check this out: Kollektivet - Don't be slappin' my penis!  I  think that they deserve much more credit than they receive.﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID          AUTHOR  \\\n",
       "397  z13kfpwrhzeuvfavb221yxoj3ynsi5vjo04    Warcorpse666     \n",
       "408  z13tczjy5xj0vjmu5231unho1ofey5zdk      LaS Music        \n",
       "324  z12rhdkrvvy3hd42c23whdeavkjdxnimg      viviane trinh    \n",
       "218  z12svt0reqvqyppzw04cgt1axrqhddcbkfc    derrick lawson   \n",
       "261  z12wuz2qqnawe50js04cejpzosrzdr0r1k40k  Dana Matich      \n",
       "\n",
       "                           DATE  \\\n",
       "397  2015-05-26T02:23:11.364000   \n",
       "408  2015-05-28T19:23:35.355000   \n",
       "324  2015-05-21T22:35:35.753000   \n",
       "218  2014-11-12T20:21:27          \n",
       "261  2014-11-08T03:32:55          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CONTENT  \\\n",
       "397  so spousal abusue cool that&#39;s great﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "408  Hey guys, I&#39;m a human.<br /><br /><br />But I don&#39;t want to be a human, I want to be a sexy fucking giraffe.<br /><br /><br />I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!<br /><br /><br />Im fucking with you, I make music, check out my first song! <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\">#giraffebruuh</a>﻿   \n",
       "324  i like the lyrics but not to music video﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "218  https://www.facebook.com/FUDAIRYQUEEN?pnref=story﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "261  Hey guys! Check this out: Kollektivet - Don't be slappin' my penis!  I  think that they deserve much more credit than they receive.﻿                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "     LABEL  VIDEO_ID  \n",
       "397  2      4         \n",
       "408  1      3         \n",
       "324  2      4         \n",
       "218  1      1         \n",
       "261  1      1         "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't truncate text fields in the display\n",
    "pd.set_option('display.max_colwidth', 0)  \n",
    "\n",
    "df_dev.sample(5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's an LF\n",
    "    * Why are they awesome\n",
    "* Can be many types:\n",
    "    * keyword\n",
    "    * pattern-match\n",
    "    * heuristic\n",
    "    * third-party models\n",
    "    * distant supervision\n",
    "    * crowdworkers (non-expert)\n",
    "* Typically an iterative process\n",
    "    * Look at examples for ideas\n",
    "    * Write an LF\n",
    "    * Check performance on dev set\n",
    "    * Balance accuracy/coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Look at examples for ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at 10 examples; got any ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>so spousal abusue cool that&amp;#39;s great﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Hey guys, I&amp;#39;m a human.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;But I don&amp;#39;t want to be a human, I want to be a sexy fucking giraffe.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Im fucking with you, I make music, check out my first song! &lt;a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\"&gt;#giraffebruuh&lt;/a&gt;﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>i like the lyrics but not to music video﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>https://www.facebook.com/FUDAIRYQUEEN?pnref=story﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Hey guys! Check this out: Kollektivet - Don't be slappin' my penis!  I  think that they deserve much more credit than they receive.﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Party Rock&lt;br /&gt;﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>You guys should check out this EXTRAORDINARY website called ZONEPA.COM .   You can make money online and start working from home today as I am!   I am making over $3,000+ per month at ZONEPA.COM !   Visit Zonepa.com and check it out!  Why does the answer rehabilitate the blushing limit? The push depreciateds the steel. How does the beautiful selection edit the range?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>like this comment then type 1337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2,124923004 wiews... wow﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CONTENT  \\\n",
       "397  so spousal abusue cool that&#39;s great﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "408  Hey guys, I&#39;m a human.<br /><br /><br />But I don&#39;t want to be a human, I want to be a sexy fucking giraffe.<br /><br /><br />I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!<br /><br /><br />Im fucking with you, I make music, check out my first song! <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\">#giraffebruuh</a>﻿   \n",
       "324  i like the lyrics but not to music video﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "218  https://www.facebook.com/FUDAIRYQUEEN?pnref=story﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "261  Hey guys! Check this out: Kollektivet - Don't be slappin' my penis!  I  think that they deserve much more credit than they receive.﻿                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "84   Party Rock<br />﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "367  Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "159  You guys should check out this EXTRAORDINARY website called ZONEPA.COM .   You can make money online and start working from home today as I am!   I am making over $3,000+ per month at ZONEPA.COM !   Visit Zonepa.com and check it out!  Why does the answer rehabilitate the blushing limit? The push depreciateds the steel. How does the beautiful selection edit the range?                                                                                                                                      \n",
       "222  like this comment then type 1337                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "343  2,124923004 wiews... wow﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "     LABEL  \n",
       "397  2      \n",
       "408  1      \n",
       "324  2      \n",
       "218  1      \n",
       "261  1      \n",
       "84   2      \n",
       "367  1      \n",
       "159  1      \n",
       "222  1      \n",
       "343  2      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display just the text and label\n",
    "df_dev[[\"CONTENT\", \"LABEL\"]].sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, x in df_dev.iterrows():\n",
    "#     if \"please\" in x.CONTENT:\n",
    "#         print(x.CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Write an LF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to create labeling functions in Snorkel is with the `@labeling_function()` decorator, which wraps a function for evaluating on a single `DataPoint` (in this case, a row of the dataframe).\n",
    "\n",
    "Looking at samples of our data, we see multiple messages where spammers are trying to get viewers to look at \"my channel\" or \"my video,\" so we write a simple LF that labels an example as spam if it includes the word \"my\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf import labeling_function\n",
    "\n",
    "# We initialize an empty list that we'll add our LFs to as we create them\n",
    "lfs = []\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if 'my' in x.CONTENT.lower() else ABSTAIN\n",
    "\n",
    "lfs.append(keyword_my)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more LFs that we've written to a collection of `DataPoints`, we use an `LFApplier`.\n",
    "\n",
    "Because our `DataPoints` are represented with a Pandas dataframe in this tutorial, we use the `PandasLFApplier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:00<00:00, 26461.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.apply import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `apply()` method is a sparse label matrix which we generally refer to as `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1586x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check performance on dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate the coverage of this LF by hand (i.e., the percentage of the dataset that it labels) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.19861286254728877\n"
     ]
    }
   ],
   "source": [
    "coverage = L_train.nnz / L_train.shape[0]\n",
    "print(f\"Coverage: {coverage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an estimate of its accuracy, we can label the development set with it and compare that to the few gold labels we do have.\n",
    "\n",
    "Note that we don't want to penalize the LF for examples where it abstained, so we filter out both the predictions and the gold labels where the prediction is `ABSTAIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 13833.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8636363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "L_dev = applier.apply(df_dev)\n",
    "L_dev_array = np.asarray(L_dev.todense()).squeeze()\n",
    "\n",
    "Y_dev = df_dev[\"LABEL\"].values\n",
    "\n",
    "accuracy = ((L_dev_array == Y_dev)[L_dev_array != ABSTAIN]).sum() / (L_dev_array != ABSTAIN).sum()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the provided `metric_score()` helper method, which allows you to specify a metric to calculate and certain classes to ignore (such as ABSTAIN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis.metrics import metric_score\n",
    "\n",
    "# Calculate accuracy, ignore all examples for which the predicted label is ABSTAIN\n",
    "# TODO: drop probs=None\n",
    "accuracy = metric_score(golds=Y_dev, preds=L_dev_array, probs=None, metric=\"accuracy\", filter_dict={\"preds\": [ABSTAIN]})\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the helper method `lf_summary()` to report the following summary statistics for multiple LFs at once:\n",
    "* Polarity: The set of labels this LF outputs\n",
    "* Coverage: The fraction of the dataset the LF labels\n",
    "* Overlaps: The fraction of the dataset where this LF and at least one other LF label\n",
    "* Conflicts: The fraction of the dataset where this LF and at least one other LF label and disagree\n",
    "* Correct: The number of `DataPoints` this LF labels correctly (if gold labels are provided)\n",
    "* Incorrect: The number of `DataPoints` this LF labels incorrectly (if gold labels are provided)\n",
    "* Emp. Acc.: The empirical accuracy of this LF (if gold labels are provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "keyword_my  0  [1]      0.22      0.0       0.0        19       3           \n",
       "\n",
       "            Emp. Acc.  \n",
       "keyword_my  0.863636   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.analysis import lf_summary\n",
    "\n",
    "lf_names= [lf.name for lf in lfs]\n",
    "lf_summary(L=L_dev, Y=Y_dev, lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Balance accuracy/coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, by looking at the examples that an LF does and doesn't label, we can get ideas for how to improve it.\n",
    "\n",
    "The helper method `error_buckets()` groups examples by their predicted label and true label, so `buckets[(1, 2)]` will contain the indices of examples that that the LF labeled 1 (SPAM) that were actually of class 2 (HAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>\"....because I AM a champion...and you're gonna hear me roar!\"   Today I AM my own champion  Today I AM a champion for the Creator  Today I AM doing positive in my world Today I AM blessing and healing all around me Today I AM successful and  creating success  ﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>I guss this song is one of my worst fears in life, to be with someone who abusive towered me and live with him.... ﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>sorry to all my haters&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;for party rock en﻿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                    CONTENT  \\\n",
       "326  \"....because I AM a champion...and you're gonna hear me roar!\"   Today I AM my own champion  Today I AM a champion for the Creator  Today I AM doing positive in my world Today I AM blessing and healing all around me Today I AM successful and  creating success  ﻿   \n",
       "61   I guss this song is one of my worst fears in life, to be with someone who abusive towered me and live with him.... ﻿                                                                                                                                                     \n",
       "284  sorry to all my haters<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />for party rock en﻿                                                                                                                                                   \n",
       "\n",
       "     LABEL  \n",
       "326  2      \n",
       "61   2      \n",
       "284  2      "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.analysis.error_analysis import error_buckets\n",
    "\n",
    "buckets = error_buckets(Y_dev, L_dev_array)\n",
    "df_dev[[\"CONTENT\", \"LABEL\"]].iloc[buckets[(1, 2)]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, `buckets[(1, 1)]` contains SPAM examples it labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Suscribe My Channel Please XD lol﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG   SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>subscribe to my channel yo - DJ Feelz﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          CONTENT  \\\n",
       "367  Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "31   Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿   \n",
       "263  Suscribe My Channel Please XD lol﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "204  SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG   SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "100  subscribe to my channel yo - DJ Feelz﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "     LABEL  \n",
       "367  1      \n",
       "31   1      \n",
       "263  1      \n",
       "204  1      \n",
       "100  1      "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[[\"CONTENT\", \"LABEL\"]].iloc[buckets[(1, 1)]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these examples, we may notice that much of the time when \"my\" is used, it's referring to \"my channel\". We can update our LF to see how making this change affects accuracy and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 16366.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  Emp. Acc.\n",
       "0  [1]      0.06      0.0       0.0        6        0          1.0      "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@labeling_function()\n",
    "def keywords_my_channel(x):\n",
    "    return SPAM if 'my channel' in x.CONTENT.lower() else ABSTAIN\n",
    "\n",
    "lfs = [keywords_my_channel]\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_dev = applier.apply(df_dev)\n",
    "lf_names= [lf.name for lf in lfs]\n",
    "lf_summary(L=L_dev, Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, accuracy does improve a bit, but it was already fairly accurate to begin with, and \"tightening\" the LF like this causes the coverage drops significantly, so we'll stick with the original LF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a single LF had high enough coverage to label our entire test dataset accurately, then we wouldn't need a classifier at all; we could just use that single simple heuristic to complete the task. But most problems are not that simple. Instead, we usually need to **combine multiple LFs** to label our dataset, both to increase the size of the generated training set (since we can't generate training labels for data points that all LFs abstained on) and to improve the overall accuracy of the training labels we generate by factoring in multiple different signals.\n",
    "\n",
    "In the following subsections, we'll show just a few of the many types of LFs that you could write to generate a training dataset for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Keyword LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text applications, some of the simplest LFs to write are often just keyword lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_my(x):\n",
    "    \"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if 'my' in x.CONTENT.lower() else ABSTAIN\n",
    "lfs.append(keyword_my)\n",
    "\n",
    "@labeling_function()\n",
    "def lf_subscribe(x):\n",
    "    \"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "    return SPAM if \"subscribe\" in x.CONTENT else 0\n",
    "lfs.append(lf_subscribe)\n",
    "\n",
    "@labeling_function()\n",
    "def lf_link(x):\n",
    "    \"\"\"Spam comments post links to other channels.\"\"\"\n",
    "    return SPAM if \"http\" in x.CONTENT.lower() else 0\n",
    "lfs.append(lf_link)\n",
    "\n",
    "@labeling_function()\n",
    "def lf_please(x):\n",
    "    \"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "    return SPAM if any([word in x.CONTENT.lower() for word in [\"please\", \"plz\"]]) else ABSTAIN\n",
    "lfs.append(lf_please)\n",
    "\n",
    "@labeling_function()\n",
    "def lf_song(x):\n",
    "    \"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "    return HAM if \"song\" in x.CONTENT.lower() else ABSTAIN\n",
    "lfs.append(lf_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Pattern-matching LFs (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a little more control over a keyword search, we can look for regular expressions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.CONTENT, flags=re.I) else ABSTAIN\n",
    "\n",
    "lfs.append(regex_check_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii.  Heuristic LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may other heuristics or \"rules of thumb\" that you come up with as you look at the data.\n",
    "So long as you can express it in a function, it's a viable LF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.CONTENT.split()) < 5 else ABSTAIN\n",
    "lfs.append(short_comment)\n",
    "\n",
    "\n",
    "# @labeling_function()\n",
    "# def short_word_lengths(x):\n",
    "#     \"\"\"Ham comments tend to have shorter words.\"\"\"\n",
    "#     words = x.CONTENT.split()\n",
    "#     lengths = [len(word) for word in words]\n",
    "#     mean_word_length = sum(lengths) / len(lengths)\n",
    "#     return HAM if mean_word_length < 4 else ABSTAIN\n",
    "# lfs.append(short_word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LFs rely on fields that aren't present in the raw data, but can be derived from it. We can enrich our data (providing more fields for the LFs to refer to) using `Preprocessors`.\n",
    "\n",
    "For example, we can use the fantastic NLP tool [spaCy](https://spacy.io/) to add lemmas, part-of-speech (pos) tags, etc. to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/braden/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the spacy english model\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.preprocess.nlp import SpacyPreprocessor\n",
    "# The SpacyPreprocessor parses the text in text_field and \n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"CONTENT\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@labeling_function(preprocessors=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "lfs.append(has_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Third-party Model LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize other models, including ones trained for other tasks that are related to, but not the same as, the one we care about.\n",
    "\n",
    "For example, the [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) tool provides a pretrained sentiment analyzer. Our spam classification task is not the same as sentiment classification, but it turns out that SPAM and HAM comments have different distributions of sentiment scores, with HAM having more positive/subjective sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "spam_polarities = [TextBlob(x.CONTENT).sentiment.polarity for i, x in df_dev.iterrows() if x.LABEL == SPAM]\n",
    "ham_polarities = [TextBlob(x.CONTENT).sentiment.polarity for i, x in df_dev.iterrows() if x.LABEL == HAM]\n",
    "\n",
    "_ = plt.hist([spam_polarities, ham_polarities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "@labeling_function()\n",
    "def textblob_polarity(x):\n",
    "    return 2 if TextBlob(x.CONTENT).sentiment.polarity > 0.3 else 0\n",
    "lfs.append(textblob_polarity)\n",
    "\n",
    "@labeling_function()\n",
    "def textblob_subjectivity(x):\n",
    "    return 2 if TextBlob(x.CONTENT).sentiment.subjectivity > 0.9 else 0\n",
    "lfs.append(textblob_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Write your own LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates just a handful of the types of LFs that one might write for this task. \n",
    "Many of these are no doubt suboptimal.\n",
    "The strength of this approach, however, is that the LF abstraction provides a flexible interface for conveying a huge variety of supervision signals, and the `LabelModel` is able to denoise these signals, reducing the need for painstaking manual fine-tuning.\n",
    "\n",
    "You can uncomment the cell below to write one or more of your own LFs.\n",
    "Don't forget to add them to the list of `lfs` so that they are included by the `LFApplier` in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @labeling_function()\n",
    "# def my_lf(x):\n",
    "#     pass\n",
    "# lfs.append(my_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our full set of LFs (including any you wrote), we can now apply these once again with `LFApplier` to get our the label matrices for the `train` and `dev` splits. We'll use the `train` split's label matrix to generate training labels with the Label Model. The `dev` split's label model is primarily helpful for looking at summary statistics.\n",
    "\n",
    "Note that the `pandas` format provides an easy interface that many practioners are familiar with, but it is also less optimized for scale. For larger datasets, more compute-intensive LFs, or larger LF sets, you may decide to use one of the other supported data formats such as `dask` or `spark` dataframes, and their corresponding applier objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:16<00:00, 94.90it/s] \n",
      "100%|██████████| 100/100 [00:00<00:00, 1053.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_link</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>6</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person</th>\n",
       "      <td>7</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>8</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.11</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>9</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "keyword_my             0  [1]      0.22      0.21      0.13       19        \n",
       "lf_subscribe           1  [1]      0.08      0.05      0.02       8         \n",
       "lf_link                2  [1]      0.10      0.07      0.05       10        \n",
       "lf_please              3  [1]      0.10      0.09      0.05       10        \n",
       "lf_song                4  [2]      0.16      0.11      0.06       11        \n",
       "regex_check_out        5  [1]      0.29      0.22      0.14       29        \n",
       "short_comment          6  [2]      0.28      0.17      0.05       19        \n",
       "has_person             7  [2]      0.15      0.12      0.04       10        \n",
       "textblob_polarity      8  [2]      0.28      0.26      0.11       18        \n",
       "textblob_subjectivity  9  [2]      0.08      0.08      0.04       5         \n",
       "\n",
       "                       Incorrect  Emp. Acc.  \n",
       "keyword_my             3          0.863636   \n",
       "lf_subscribe           0          1.000000   \n",
       "lf_link                0          1.000000   \n",
       "lf_please              0          1.000000   \n",
       "lf_song                5          0.687500   \n",
       "regex_check_out        0          1.000000   \n",
       "short_comment          9          0.678571   \n",
       "has_person             5          0.666667   \n",
       "textblob_polarity      10         0.642857   \n",
       "textblob_subjectivity  3          0.625000   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "lf_names= [lf.name for lf in lfs]\n",
    "lf_summary(L=L_dev, Y=Y_dev, lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our labeling functions vary in coverage, accuracy, and how much they overlap/conflict with one another.\n",
    "We can view a histogram of how many weak labels the `DataPoints` in our dev set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASUklEQVR4nO3db2xd913H8feHlHSwweion5CEOhsZLGPQIi8bTBSJtWumomQSm5ahoYIqRUUNFAqCDFAnZULqBprgQWCNWNAElFBaHlgsEMb+SdPUzu5aVpIS1c1K4zA0s5QN2NY07ZcHPoUb49Qn8XWu+/P7JVk9v38333vUfHxyzrnnpqqQJLXrW0ZdgCRpZRn0ktQ4g16SGmfQS1LjDHpJatxloy5goSuvvLLGx8dHXYYkvag8+OCD/15VY4uNrbqgHx8fZ3p6etRlSNKLSpJ/Od9Yr1M3SbYnOZ5kJsneF5j300kqycRA33u6dceT3HBhpUuSlmvJI/ok64D9wPXALDCVZLKqji2Y9x3AbcADA31bgV3Aa4HvAf4hyaur6tnhvQVJ0gvpc0S/DZipqhNVdQY4BOxcZN77gPcD3xzo2wkcqqqnq+qLwEz3epKkS6RP0G8ATg60Z7u+/5XkR4BNVfXRC10rSVpZy769Msm3AB8EfnUZr7E7yXSS6bm5ueWWJEka0CfoTwGbBtobu77nfQfwg8CnkjwBvBGY7C7ILrUWgKo6UFUTVTUxNrbo3UGSpIvUJ+ingC1JNidZz/zF1cnnB6vqq1V1ZVWNV9U4cD+wo6qmu3m7klyeZDOwBfjc0N+FJOm8lrzrpqrOJtkDHAHWAQer6miSfcB0VU2+wNqjSe4BjgFngVu940aSLq2stufRT0xMlB+YkqQLk+TBqppYbGzVfTK2BeN7F958NBpP3HnjqEuQtAr4UDNJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXK+gT7I9yfEkM0n2LjJ+S5JHkjyc5DNJtnb940m+0fU/nORDw34DkqQXtuRXCSZZB+wHrgdmgakkk1V1bGDa3VX1oW7+DuCDwPZu7PGqunq4ZUuS+upzRL8NmKmqE1V1BjgE7BycUFVfG2i+FFhd3zguSWtYn6DfAJwcaM92fedIcmuSx4EPAL80MLQ5yUNJPp3kxxf7A5LsTjKdZHpubu4CypckLWVoF2Oran9VvQr4DeC3u+4vAd9bVdcAtwN3J/nORdYeqKqJqpoYGxsbVkmSJPoF/Slg00B7Y9d3PoeAtwFU1dNV9ZVu+0HgceDVF1eqJOli9An6KWBLks1J1gO7gMnBCUm2DDRvBB7r+se6i7kkeSWwBTgxjMIlSf0seddNVZ1Nsgc4AqwDDlbV0ST7gOmqmgT2JLkOeAZ4CripW34tsC/JM8BzwC1VdXol3ogkaXFLBj1AVR0GDi/ou2Ng+7bzrLsPuG85BUqSlsdPxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9ku1JjieZSbJ3kfFbkjyS5OEkn0mydWDsPd2640luGGbxkqSlLRn03Zd77wfeCmwF3jUY5J27q+p1VXU18AHgg93arcx/mfhrge3AHz7/ZeGSpEujzxH9NmCmqk5U1RngELBzcEJVfW2g+VKguu2dwKGqerqqvgjMdK8nSbpE+nw5+Abg5EB7FnjDwklJbgVuB9YDPzmw9v4FazdcVKWSpIsytIuxVbW/ql4F/Abw2xeyNsnuJNNJpufm5oZVkiSJfkF/Ctg00N7Y9Z3PIeBtF7K2qg5U1URVTYyNjfUoSZLUV5+gnwK2JNmcZD3zF1cnByck2TLQvBF4rNueBHYluTzJZmAL8Lnlly1J6mvJc/RVdTbJHuAIsA44WFVHk+wDpqtqEtiT5DrgGeAp4KZu7dEk9wDHgLPArVX17Aq9F0nSIvpcjKWqDgOHF/TdMbB92wus/R3gdy62QEnS8vjJWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF6PKdaL0/jej466BACeuPPGUZcgrWke0UtS4wx6SWqcQS9JjesV9Em2JzmeZCbJ3kXGb09yLMkXknw8yVUDY88mebj7mVy4VpK0spa8GJtkHbAfuB6YBaaSTFbVsYFpDwETVfX1JL8AfAB4Zzf2jaq6esh1S5J66nNEvw2YqaoTVXUGOATsHJxQVZ+sqq93zfuBjcMtU5J0sfoE/Qbg5EB7tus7n5uBvx1ovyTJdJL7k7xtsQVJdndzpufm5nqUJEnqa6j30Sd5NzAB/MRA91VVdSrJK4FPJHmkqh4fXFdVB4ADABMTEzXMmiRpretzRH8K2DTQ3tj1nSPJdcBvATuq6unn+6vqVPffE8CngGuWUa8k6QL1CfopYEuSzUnWA7uAc+6eSXINcBfzIf/lgf4rklzebV8JvAkYvIgrSVphS566qaqzSfYAR4B1wMGqOppkHzBdVZPA7wIvA/4qCcCTVbUDeA1wV5LnmP+lcueCu3UkSSus1zn6qjoMHF7Qd8fA9nXnWfdZ4HXLKVCStDx+MlaSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuN6BX2S7UmOJ5lJsneR8duTHEvyhSQfT3LVwNhNSR7rfm4aZvGSpKUtGfRJ1gH7gbcCW4F3Jdm6YNpDwERV/RBwL/CBbu0rgPcCbwC2Ae9NcsXwypckLaXPEf02YKaqTlTVGeAQsHNwQlV9sqq+3jXvBzZ22zcAH6uq01X1FPAxYPtwSpck9dEn6DcAJwfas13f+dwM/O2FrE2yO8l0kum5ubkeJUmS+hrqxdgk7wYmgN+9kHVVdaCqJqpqYmxsbJglSdKa1yfoTwGbBtobu75zJLkO+C1gR1U9fSFrJUkrp0/QTwFbkmxOsh7YBUwOTkhyDXAX8yH/5YGhI8BbklzRXYR9S9cnSbpELltqQlWdTbKH+YBeBxysqqNJ9gHTVTXJ/KmalwF/lQTgyaraUVWnk7yP+V8WAPuq6vSKvBNJ0qKWDHqAqjoMHF7Qd8fA9nUvsPYgcPBiC5QkLY+fjJWkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS43o91OzFZHzvR0ddgiStKh7RS1LjDHpJapxBL0mNM+glqXG9gj7J9iTHk8wk2bvI+LVJPp/kbJK3Lxh7NsnD3c/kwrWSpJW15F03SdYB+4HrgVlgKslkVR0bmPYk8HPAry3yEt+oqquHUKsk6SL0ub1yGzBTVScAkhwCdgL/G/RV9UQ39twK1ChJWoY+p242ACcH2rNdX18vSTKd5P4kb1tsQpLd3Zzpubm5C3hpSdJSLsXF2KuqagL4GeD3k7xq4YSqOlBVE1U1MTY2dglKkqS1o0/QnwI2DbQ3dn29VNWp7r8ngE8B11xAfZKkZeoT9FPAliSbk6wHdgG97p5JckWSy7vtK4E3MXBuX5K08pYM+qo6C+wBjgCPAvdU1dEk+5LsAEjy+iSzwDuAu5Ic7Za/BphO8o/AJ4E7F9ytI0laYb0ealZVh4HDC/ruGNieYv6UzsJ1nwVet8waJUnL4CdjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXHNfDq7VZ7V8YfsTd9446hKkkfCIXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9ku1JjieZSbJ3kfFrk3w+ydkkb18wdlOSx7qfm4ZVuCSpnyWDPsk6YD/wVmAr8K4kWxdMexL4OeDuBWtfAbwXeAOwDXhvkiuWX7Ykqa8+R/TbgJmqOlFVZ4BDwM7BCVX1RFV9AXhuwdobgI9V1emqegr4GLB9CHVLknrqE/QbgJMD7dmur49ea5PsTjKdZHpubq7nS0uS+lgVF2Or6kBVTVTVxNjY2KjLkaSm9An6U8CmgfbGrq+P5ayVJA1Bn6CfArYk2ZxkPbALmOz5+keAtyS5orsI+5auT5J0iSwZ9FV1FtjDfEA/CtxTVUeT7EuyAyDJ65PMAu8A7kpytFt7Gngf878spoB9XZ8k6RLp9VWCVXUYOLyg746B7SnmT8sstvYgcHAZNUqSlmFVXIyVJK0cg16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LheQZ9ke5LjSWaS7F1k/PIkf9mNP5BkvOsfT/KNJA93Px8abvmSpKUs+VWCSdYB+4HrgVlgKslkVR0bmHYz8FRVfV+SXcD7gXd2Y49X1dVDrluS1FOfI/ptwExVnaiqM8AhYOeCOTuBj3Tb9wJvTpLhlSlJulh9gn4DcHKgPdv1LTqnqs4CXwW+uxvbnOShJJ9O8uOL/QFJdieZTjI9Nzd3QW9AkvTCVvpi7JeA762qa4DbgbuTfOfCSVV1oKomqmpibGxshUuSpLWlT9CfAjYNtDd2fYvOSXIZ8HLgK1X1dFV9BaCqHgQeB1693KIlSf31CfopYEuSzUnWA7uAyQVzJoGbuu23A5+oqkoy1l3MJckrgS3AieGULknqY8m7bqrqbJI9wBFgHXCwqo4m2QdMV9Uk8GHgT5PMAKeZ/2UAcC2wL8kzwHPALVV1eiXeiLSU8b0fHXUJADxx542jLkFrzJJBD1BVh4HDC/ruGNj+JvCORdbdB9y3zBolScvgJ2MlqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY3r9cUjkobHb7rSpeYRvSQ1rlfQJ9me5HiSmSR7Fxm/PMlfduMPJBkfGHtP1388yQ3DK12S1MeSp26SrAP2A9cDs8BUksmqOjYw7Wbgqar6viS7gPcD70yylfkvCn8t8D3APyR5dVU9O+w3IunCrIZTSJ4+ujT6HNFvA2aq6kRVnQEOATsXzNkJfKTbvhd4c5J0/Yeq6umq+iIw072eJOkS6XMxdgNwcqA9C7zhfHOq6mySrwLf3fXfv2DthoV/QJLdwO6u+V9JjveqfnFXAv++jPUtcV+cy/1xrpHvj7x/lH/6OUa+L4bgqvMNrIq7bqrqAHBgGK+VZLqqJobxWi927otzuT/O5f74P63viz6nbk4BmwbaG7u+ReckuQx4OfCVnmslSSuoT9BPAVuSbE6ynvmLq5ML5kwCN3Xbbwc+UVXV9e/q7srZDGwBPjec0iVJfSx56qY7574HOAKsAw5W1dEk+4DpqpoEPgz8aZIZ4DTzvwzo5t0DHAPOArdegjtuhnIKqBHui3O5P87l/vg/Te+LzB94S5Ja5SdjJalxBr0kNa6ZoF/qMQ1rSZJNST6Z5FiSo0luG3VNo5ZkXZKHkvzNqGsZtSTfleTeJP+c5NEkPzrqmkYpya90f0/+KclfJHnJqGsatiaCfuAxDW8FtgLv6h6/sFadBX61qrYCbwRuXeP7A+A24NFRF7FK/AHwd1X1A8APs4b3S5INwC8BE1X1g8zfcLJrtFUNXxNBT7/HNKwZVfWlqvp8t/2fzP9F/n+fSF4rkmwEbgT+eNS1jFqSlwPXMn+nHFV1pqr+Y7RVjdxlwLd1nwH6duBfR1zP0LUS9Is9pmHNBtug7kmi1wAPjLaSkfp94NeB50ZdyCqwGZgD/qQ7lfXHSV466qJGpapOAb8HPAl8CfhqVf39aKsavlaCXotI8jLgPuCXq+pro65nFJL8FPDlqnpw1LWsEpcBPwL8UVVdA/w3sGavaSW5gvl//W9m/gm7L03y7tFWNXytBL2PWlggybcyH/J/XlV/Pep6RuhNwI4kTzB/Su8nk/zZaEsaqVlgtqqe/xfevcwH/1p1HfDFqpqrqmeAvwZ+bMQ1DV0rQd/nMQ1rRveI6A8Dj1bVB0ddzyhV1XuqamNVjTP//8Unqqq5I7a+qurfgJNJvr/rejPzn1xfq54E3pjk27u/N2+mwYvTq+Lplct1vsc0jLisUXoT8LPAI0ke7vp+s6oOj7AmrR6/CPx5d1B0Avj5EdczMlX1QJJ7gc8zf7faQzT4OAQfgSBJjWvl1I0k6TwMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4/wHzVVuxSILKKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Move plot_label_frequency() to core snorkel repo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist(np.asarray((L != 0).sum(axis=1)), density=True, bins=range(L.shape[1]))    \n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that over half of our training dataset `DataPoints` have 0 or 1 weak labels. \n",
    "Fortunately, the signal we do have can be used to train a classifier with a larger feature set than just these labeling functions that we've created, allowing it to generalize beyond what we've specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine with Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to convert these many weak labels into a single _noise-aware_ probabilistic (or confidence-weighted) label per `DataPoint`.\n",
    "A simple baseline for doing this is to take the majority vote on a per-`DataPoint` basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.81}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "mv_model = MajorityLabelVoter()\n",
    "mv_model.score(L_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we can clearly see by looking the summary statistics of our LFs, they are not all equally accurate, and should ideally not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. To handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine our weak labels.\n",
    "\n",
    "This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task. For more technical details of this overall approach, see our [NeurIPS 2016](https://arxiv.org/abs/1605.07723) and [AAAI 2019](https://arxiv.org/abs/1810.02840) papers.\n",
    "\n",
    "Note that no gold labels are used during the training process; the `LabelModel` is able to estimate the accuracies of the labeling functions using only the weak label matrix as input. (See the [TODO: dependency learning](TBD) tutorial for a demonstration of how to learn correlations as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epochs]: TRAIN:[loss=0.129]\n",
      "[26 epochs]: TRAIN:[loss=0.022]\n",
      "[51 epochs]: TRAIN:[loss=0.013]\n",
      "[76 epochs]: TRAIN:[loss=0.011]\n",
      "[101 epochs]: TRAIN:[loss=0.010]\n",
      "[126 epochs]: TRAIN:[loss=0.009]\n",
      "[151 epochs]: TRAIN:[loss=0.009]\n",
      "[176 epochs]: TRAIN:[loss=0.009]\n",
      "[201 epochs]: TRAIN:[loss=0.009]\n",
      "[226 epochs]: TRAIN:[loss=0.009]\n",
      "[251 epochs]: TRAIN:[loss=0.008]\n",
      "[276 epochs]: TRAIN:[loss=0.008]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# TODO: get more frequent logging statement printing\n",
    "label_model = LabelModel(cardinality=2, verbose=True, seed=123)\n",
    "label_model.train_model(L_train, n_epochs=300, log_train_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that our resulting predicted labels are probabilistic, with the points we are least certain about having labels close to 0.5. The following histogram shows the confidences we have that each `DataPoint` has the label SPAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQAElEQVR4nO3df6zddX3H8edLKro5R/lRG9KWXYx1jrgo5IbVuDi10wgslmRKMHN0pFmjY8bFJVs3/9jPP+CPySQxbI04i/EHjM3RCHNjBUJmVrQM5KeOK4PRrtArQjdH/MF874/zwV1qb+9p77nntJ8+H8nN+Xw/38+53/en5/bV7/2c7/k2VYUkqS8vmnQBkqTRM9wlqUOGuyR1yHCXpA4Z7pLUoWWTLgDgtNNOq6mpqUmXIUnHlLvuuuubVbXiYPuOinCfmppi165dky5Dko4pSR6bb5/LMpLUIcNdkjpkuEtShwx3SerQUOGeZHmSG5J8LclDSd6Q5JQktyR5uD2e3MYmyVVJZpLcm+ScpZ2CJOlAw565fxT4YlW9Bngd8BCwBdhRVWuBHW0b4DxgbfvaDFw90oolSQtaMNyTnAS8CbgGoKq+V1XPABuAbW3YNuDC1t4AXFsDO4HlSU4feeWSpHkNc+Z+JjAL/FWSu5N8PMnLgJVVtbeNeQJY2dqrgMfnPH9363uBJJuT7Eqya3Z29shnIEn6EcOE+zLgHODqqjob+B/+fwkGgBrcFP6wbgxfVVurarqqplesOOgHrCRJR2iYT6juBnZX1Z1t+wYG4f5kktOram9bdtnX9u8B1sx5/urWJ0lHpaktN03s2I9efsGSfN8Fz9yr6gng8SQ/3brWAw8C24GNrW8jcGNrbwcuaVfNrAP2z1m+kSSNwbD3lvkA8OkkJwKPAJcy+Ifh+iSbgMeAi9rYm4HzgRng2TZWkjRGQ4V7Vd0DTB9k1/qDjC3gskXWJUlaBD+hKkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUNDhXuSR5Pcl+SeJLta3ylJbknycHs8ufUnyVVJZpLcm+ScpZyAJOlHHc6Z+1uq6vVVNd22twA7qmotsKNtA5wHrG1fm4GrR1WsJGk4i1mW2QBsa+1twIVz+q+tgZ3A8iSnL+I4kqTDNGy4F/CPSe5Ksrn1rayqva39BLCytVcBj8957u7W9wJJNifZlWTX7OzsEZQuSZrPsiHH/XxV7UnyCuCWJF+bu7OqKkkdzoGraiuwFWB6evqwnitJOrShztyrak973Ad8HjgXePL55Zb2uK8N3wOsmfP01a1PkjQmC4Z7kpclefnzbeDtwP3AdmBjG7YRuLG1twOXtKtm1gH75yzfSJLGYJhlmZXA55M8P/4zVfXFJF8Brk+yCXgMuKiNvxk4H5gBngUuHXnVkqRDWjDcq+oR4HUH6X8KWH+Q/gIuG0l1kqQj4idUJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdWjocE9yQpK7k3yhbZ+Z5M4kM0muS3Ji639J255p+6eWpnRJ0nwO58z9g8BDc7avAK6sqlcBTwObWv8m4OnWf2UbJ0kao6HCPclq4ALg4207wFuBG9qQbcCFrb2hbdP2r2/jJUljMuyZ+58DvwP8oG2fCjxTVc+17d3AqtZeBTwO0Pbvb+NfIMnmJLuS7JqdnT3C8iVJB7NguCf5JWBfVd01ygNX1daqmq6q6RUrVozyW0vScW/ZEGPeCLwzyfnAS4GfBD4KLE+yrJ2drwb2tPF7gDXA7iTLgJOAp0ZeuSRpXgueuVfV71XV6qqaAi4Gbq2qXwFuA97Vhm0Ebmzt7W2btv/WqqqRVi1JOqTFXOf+u8CHkswwWFO/pvVfA5za+j8EbFlciZKkwzXMsswPVdXtwO2t/Qhw7kHGfAd49whqkyQdIT+hKkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDh3X7AWlSprbcNJHjPnr5BRM5rrRYnrlLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4tGO5JXprky0m+muSBJH/U+s9McmeSmSTXJTmx9b+kbc+0/VNLOwVJ0oGGOXP/LvDWqnod8HrgHUnWAVcAV1bVq4CngU1t/Cbg6dZ/ZRsnSRqjBcO9Br7dNl/cvgp4K3BD698GXNjaG9o2bf/6JBlZxZKkBQ215p7khCT3APuAW4BvAM9U1XNtyG5gVWuvAh4HaPv3A6eOsmhJ0qENFe5V9b9V9XpgNXAu8JrFHjjJ5iS7kuyanZ1d7LeTJM1xWFfLVNUzwG3AG4DlSZ7/P1hXA3taew+wBqDtPwl46iDfa2tVTVfV9IoVK46wfEnSwQxztcyKJMtb+8eAtwEPMQj5d7VhG4EbW3t726btv7WqapRFS5IObdnCQzgd2JbkBAb/GFxfVV9I8iDwuSR/CtwNXNPGXwN8KskM8C3g4iWoW5J0CAuGe1XdC5x9kP5HGKy/H9j/HeDdI6lOknRE/ISqJHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDi0Y7knWJLktyYNJHkjywdZ/SpJbkjzcHk9u/UlyVZKZJPcmOWepJyFJeqFhztyfA367qs4C1gGXJTkL2ALsqKq1wI62DXAesLZ9bQauHnnVkqRDWjDcq2pvVf1ra/838BCwCtgAbGvDtgEXtvYG4Noa2AksT3L6yCuXJM1r2eEMTjIFnA3cCaysqr1t1xPAytZeBTw+52m7W9/eOX0k2czgzJ4zzjjjMMvWpExtuWnSJUgawtBvqCb5CeBvgN+qqv+au6+qCqjDOXBVba2q6aqaXrFixeE8VZK0gKHCPcmLGQT7p6vqb1v3k88vt7THfa1/D7BmztNXtz5J0pgMc7VMgGuAh6rqI3N2bQc2tvZG4MY5/Ze0q2bWAfvnLN9IksZgmDX3NwK/CtyX5J7W9/vA5cD1STYBjwEXtX03A+cDM8CzwKUjrViStKAFw72q/hnIPLvXH2R8AZctsi5J0iL4CVVJ6pDhLkkdMtwlqUOGuyR1yHCXpA4d1u0HjkaT/Dj8o5dfMLFjS9KheOYuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOnTMXwo5SZO6DNNLMCUtxDN3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDi0Y7kk+kWRfkvvn9J2S5JYkD7fHk1t/klyVZCbJvUnOWcriJUkHN8yZ+yeBdxzQtwXYUVVrgR1tG+A8YG372gxcPZoyJUmHY8Fwr6o7gG8d0L0B2Nba24AL5/RfWwM7geVJTh9VsZKk4RzpmvvKqtrb2k8AK1t7FfD4nHG7W9+PSLI5ya4ku2ZnZ4+wDEnSwSz6DdWqKqCO4Hlbq2q6qqZXrFix2DIkSXMcabg/+fxyS3vc1/r3AGvmjFvd+iRJY3Sk4b4d2NjaG4Eb5/Rf0q6aWQfsn7N8I0kakwX/g+wknwXeDJyWZDfwB8DlwPVJNgGPARe14TcD5wMzwLPApUtQsyRpAQuGe1W9Z55d6w8ytoDLFluUJGlx/ISqJHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4tePsBHX2mttw06RIkHeU8c5ekDhnuktQhw12SOmS4S1KHDHdJ6pBXy0g6angl2Oh45i5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI65KWQ0lFqUpcFPnr5BRM5rkbLM3dJ6pDhLkkdWpJwT/KOJF9PMpNky1IcQ5I0v5GvuSc5AfgY8DZgN/CVJNur6sFRH0vS6HkLgD4sxRuq5wIzVfUIQJLPARsAw13HHINOx6qlCPdVwONztncDP3fgoCSbgc1t89tJvn7AkNOAby5BfccC5358cu7HoVyxqLn/1Hw7JnYpZFVtBbbOtz/JrqqaHmNJRw3n7tyPN8599HNfijdU9wBr5myvbn2SpDFZinD/CrA2yZlJTgQuBrYvwXEkSfMY+bJMVT2X5DeBfwBOAD5RVQ8cwbead8nmOODcj0/O/fi0JHNPVS3F95UkTZCfUJWkDhnuktShiYf7QrcqSPKSJNe1/XcmmRp/lUtjiLl/KMmDSe5NsiPJvNe0HmuGvUVFkl9OUkm6uUxumLknuai99g8k+cy4a1wqQ/zMn5HktiR3t5/78ydR56gl+USSfUnun2d/klzV/lzuTXLOog9aVRP7YvCG6zeAVwInAl8FzjpgzG8Af9HaFwPXTbLmMc/9LcCPt/b7j6e5t3EvB+4AdgLTk657jK/7WuBu4OS2/YpJ1z3GuW8F3t/aZwGPTrruEc39TcA5wP3z7D8f+HsgwDrgzsUec9Jn7j+8VUFVfQ94/lYFc20AtrX2DcD6JBljjUtlwblX1W1V9Wzb3MngMwM9GOZ1B/gT4ArgO+MsbokNM/dfBz5WVU8DVNW+Mde4VIaZewE/2donAf85xvqWTFXdAXzrEEM2ANfWwE5geZLTF3PMSYf7wW5VsGq+MVX1HLAfOHUs1S2tYeY+1yYG/7L3YMG5t19L11RVbzd3GeZ1fzXw6iRfSrIzyTvGVt3SGmbufwi8N8lu4GbgA+MpbeIONw8W5P/EdAxI8l5gGviFSdcyDkleBHwE+LUJlzIpyxgszbyZwW9rdyT52ap6ZqJVjcd7gE9W1Z8leQPwqSSvraofTLqwY82kz9yHuVXBD8ckWcbgV7WnxlLd0hrqNg1JfhH4MPDOqvrumGpbagvN/eXAa4HbkzzKYA1yeydvqg7zuu8GtlfV96vq34F/YxD2x7ph5r4JuB6gqv4FeCmDm4r1buS3bZl0uA9zq4LtwMbWfhdwa7V3II5xC849ydnAXzII9l7WXWGBuVfV/qo6raqmqmqKwfsN76yqXZMpd6SG+Zn/OwZn7SQ5jcEyzSPjLHKJDDP3/wDWAyT5GQbhPjvWKidjO3BJu2pmHbC/qvYu6jseBe8in8/gzOQbwIdb3x8z+MsMgxf3r4EZ4MvAKydd8xjn/k/Ak8A97Wv7pGse19wPGHs7nVwtM+TrHgbLUg8C9wEXT7rmMc79LOBLDK6kuQd4+6RrHtG8PwvsBb7P4DezTcD7gPfNec0/1v5c7hvFz7u3H5CkDk16WUaStAQMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktSh/wNpsI1grnsz6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_probabilities_histogram(Y_probs):\n",
    "    plt.hist(Y_probs[:,0])\n",
    "    \n",
    "Y_probs_train = label_model.predict_proba(L_train)\n",
    "plot_probabilities_histogram(Y_probs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.82}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_model.score(L_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our `LabelModel` does improve over the majority vote baseline, it is still somewhat limited as a classifier. \n",
    "For example, many of our `DataPoints` have few or no LFs voting on them. \n",
    "We will now train a discriminative classifier with this training set to see if we can improve performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict with Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now train classifier\n",
    "    * Can use any third-party classifier (plug into your existing pipelines!)\n",
    "    * Some libraries natively support probabilistic labels (us, TF); for others, can round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert label convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis.utils import probs_to_preds, convert_labels\n",
    "\n",
    "# Y_train = df_train['LABEL'].map({1: 1, 2: 0}) # This will not be available\n",
    "Y_train = Y_probs_train\n",
    "Y_dev = convert_labels(df_dev['LABEL'].values, \"categorical\", \"onezero\")\n",
    "Y_valid = convert_labels(df_valid['LABEL'].values, \"categorical\", \"onezero\")\n",
    "Y_test = convert_labels(df_test['LABEL'].values, \"categorical\", \"onezero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use bag-of-ngrams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "words_train = [row.CONTENT for i, row in df_train.iterrows()]\n",
    "words_dev = [row.CONTENT for i, row in df_dev.iterrows()]\n",
    "words_valid = [row.CONTENT for i, row in df_valid.iterrows()]\n",
    "words_test = [row.CONTENT for i, row in df_test.iterrows()]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))               \n",
    "X_train = vectorizer.fit_transform(words_train)\n",
    "X_dev = vectorizer.transform(words_dev)\n",
    "X_valid = vectorizer.transform(words_valid)\n",
    "X_test = vectorizer.transform(words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter out examples with no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.asarray(L_train.sum(axis=1) > 0).squeeze()\n",
    "X_train = X_train[mask, :]\n",
    "Y_train = Y_train[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# import simple logistic regression classifier in Keras\n",
    "# train on noise-aware loss\n",
    "# score\n",
    "# bonus: peek at performance w/r/t using hard (int) labels i/o soft (float) labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TBD\n",
    "* Rounding to hard labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_rounded = convert_labels(probs_to_preds(Y_train), \"categorical\", \"onezero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/braden/repos/snorkel/.env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8378378378378378"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(X_train, Y_train_rounded)\n",
    "\n",
    "sklearn_model.score(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare with training on dev directly (see, we did better)\n",
    "    * And we could do even better with more raw unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/braden/repos/snorkel/.env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8864864864864865"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev = vectorizer.fit_transform(words_dev)\n",
    "X_valid = vectorizer.transform(words_valid)\n",
    "X_test = vectorizer.transform(words_test)\n",
    "\n",
    "sklearn_model.fit(X_dev, Y_dev)\n",
    "sklearn_model.score(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With all ablations done, now evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "snorkel",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
