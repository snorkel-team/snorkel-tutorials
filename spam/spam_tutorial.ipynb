{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Snorkel Tutorial: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the process of using `Snorkel` to classify YouTube comments as `SPAM` or `HAM` (not spam).\n",
    "For an overview of Snorkel, visit [snorkel.org](http://snorkel.org).\n",
    "You can also check out the [Snorkel API documentation](https://snorkel.readthedocs.io/).\n",
    "\n",
    "For our task, we have access to a large amount of *unlabeled data*, which can be prohibitively expensive and slow to label manually.\n",
    "We therefore turn to weak supervision using **_labeling functions_**, or noisy, programmatic heuristics, to assign labels to unlabeled training data efficiently.\n",
    "We also have access to a small amount of labeled data, which we only use for evaluation purposes.\n",
    "\n",
    "The tutorial is divided into four parts:\n",
    "1. **Loading Data**: We load a [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle.\n",
    "\n",
    "2. **Writing Labeling Functions**: We write Python programs that take as input a data point and assign labels (or abstain) using heuristics, pattern matching, and third-party models.\n",
    "\n",
    "3. **Combining Weak Labels with the Label Model**: We use the outputs of the labeling functions over the training set as input to the label model, which assings probabilistic labels to the training set.\n",
    "\n",
    "4. **Training a Classifier**: We train a classifier that can predict labels for *any* YouTube comment (not just the ones labeled by the labeling functions) using the probabilistic training labels from step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) that consists of YouTube comments from 5 videos. The task is to classify each comment as being\n",
    "\n",
    "* **`SPAM`**: irrelevant or inappropriate messages, or\n",
    "* **`HAM`**: comments relevant to the video\n",
    "\n",
    "For example, the following comments are `SPAM`:\n",
    "\n",
    "        \"Subscribe to me for free Android games, apps..\"\n",
    "\n",
    "        \"Please check out my vidios\"\n",
    "\n",
    "        \"Subscribe to me and I'll subscribe back!!!\"\n",
    "\n",
    "and these are `HAM`:\n",
    "\n",
    "        \"3:46 so cute!\"\n",
    "\n",
    "        \"This looks so fun and it's a good song\"\n",
    "\n",
    "        \"This is a weird video.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits in Snorkel\n",
    "\n",
    "We split our data into 4 sets:\n",
    "* **Training Set**: The largest split of the dataset. We do not have ground truth or \"gold\" labels for these data points; we will be generating their labels with weak supervision.\n",
    "* \\[Optional\\] **Development Set**: A small labeled subset of the training data (e.g. 100 points) to guide LF iteration. See note below.\n",
    "* **Validation Set**: A labeled set used to tune hyperparameters and/or perform early stopping while training the classifier.\n",
    "* **Test Set**: A labeled set for final evaluation of our classifier. This set should only be used for final evaluation, _not_ error analysis.\n",
    "\n",
    "\n",
    "While it is possible to develop labeling functions on the unlabeled training set only, users often find it more time-efficient to label a small dev set to provide a quick approximate signal on the accuracies and failure modes of their LFs (rather than scrolling through training examples and mentally assessing approximate accuracy).\n",
    "Alternatively, users sometimes will have the validation set also serve as the development set.\n",
    "Do the latter only with caution: because the labeling functions will be based on examples from the validation set, the validation set will no longer be an unbiased proxy for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Kaggle dataset and create Pandas DataFrame objects for each of the sets described above.\n",
    "DataFrames are extremely popular in Python data analysis workloads, and Snorkel provides native support\n",
    "for several DataFrame-like data structures, including Pandas, Dask, and PySpark.\n",
    "For more information on working with Pandas DataFrames, see the [Pandas DataFrame guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html).\n",
    "\n",
    "Each DataFrame consists of the following fields:\n",
    "* **`author`**: Username of the comment author\n",
    "* **`data`**: Date and time the comment was posted\n",
    "* **`text`**: Raw text content of the comment\n",
    "* **`label`**: Whether the comment is `SPAM` (1), `HAM` (0), or `UNKNOWN/ABSTAIN` (-1)\n",
    "* **`video`**: Video the comment is associated with\n",
    "\n",
    "We start by loading our data.\n",
    "The `load_spam_dataset()` method downloads the raw CSV files from the internet, divides them into splits, converts them into DataFrames, and shuffles them.\n",
    "As mentioned above, the dataset contains comments from 5 of the most popular YouTube videos during a period between 2014 and 2015.\n",
    "* The first four videos' comments are combined to form the `train` set. This set has no gold labels.\n",
    "* The `dev` set is a random sample of 200 data points from the `train` set with gold labels added.\n",
    "* The fifth video is split 50/50 between a validation set (`valid`) and `test` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, df_dev, df_valid, df_test = load_spam_dataset()\n",
    "\n",
    "# We pull out the label vectors for ease of use later\n",
    "Y_dev = df_dev[\"label\"].values\n",
    "Y_valid = df_valid[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Pepe The Meme King</td>\n",
       "      <td>2015-05-19T03:49:29.427000</td>\n",
       "      <td>everyday I&amp;#39;m shufflin﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Melissa Erhart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Angel</td>\n",
       "      <td>2014-11-02T17:27:09</td>\n",
       "      <td>Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sandeep Singh</td>\n",
       "      <td>2015-05-23T17:51:58.957000</td>\n",
       "      <td>Charlie from LOST﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>BigBird Larry</td>\n",
       "      <td>2015-05-24T09:48:00.835000</td>\n",
       "      <td>Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                        date  \\\n",
       "128  Pepe The Meme King  2015-05-19T03:49:29.427000   \n",
       "151  Melissa Erhart      NaN                          \n",
       "31   Angel               2014-11-02T17:27:09          \n",
       "29   Sandeep Singh       2015-05-23T17:51:58.957000   \n",
       "237  BigBird Larry       2015-05-24T09:48:00.835000   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "128  everyday I&#39;m shufflin﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "151  Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "31   Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿   \n",
       "29   Charlie from LOST﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "237  Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "     label  video  \n",
       "128  0      3      \n",
       "151  1      4      \n",
       "31   1      1      \n",
       "29   0      4      \n",
       "237  0      4      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "\n",
    "df_dev.sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution varies slightly from class to class, but all are approximately class-balanced.\n",
    "You can verify this by looking at the dev set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV    54.0% SPAM\n",
      "VALID  46.7% SPAM\n",
      "TEST   47.2% SPAM\n"
     ]
    }
   ],
   "source": [
    "# For clarity, we define constants to represent the class labels for spam, ham, and abstaining.\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "for split_name, df in [(\"dev\", df_dev), (\"valid\", df_valid), (\"test\", df_test)]:\n",
    "    spam_freq = (df[\"label\"].values == SPAM).mean()\n",
    "    print(f\"{split_name.upper():<6} {spam_freq * 100:0.1f}% SPAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeling functions (LFs) help users encode domain knowledge and other supervision sources programmatically.**\n",
    "\n",
    "LFs are heuristics that take as input a data point and either assign a label to it (in this case, `HAM` or `SPAM`) or abstain (don't assign any label). Labeling functions can be *noisy*: they don't have perfect accuracy and don't have to label every data point.\n",
    "Moreover, different labeling functions can overlap (label the same data point) and even conflict (assign different labels to the same data point). This is expected, and we demonstrate how we deal with this later.\n",
    "\n",
    "Because their only requirement is that they map a data point a label (or abstain), they can wrap a wide variety of forms of supervision. Examples include, but are not limited to:\n",
    "* *Keyword searches*: looking for specific words in a sentence\n",
    "* *Pattern matching*: looking for specific syntactical patterns\n",
    "* *Third-party models*: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "* *Distant supervision*: using external knowledge base\n",
    "* *Crowdworker labels*: treating each crowdworker as a black-box function that assigns labels to subsets of the data\n",
    "\n",
    "The process of **developing LFs** is iterative and usually consists of:\n",
    "* Writing a function\n",
    "* Estimating its performance by looking at labeled examples in the training set or dev set\n",
    "* Iterating to improve coverage or accuracy as necessary.\n",
    "\n",
    "Balancing accuracy and coverage for specific labeling functions as well as the overall set of LFs developed is often a trade-off, and depending on the use case, users may tend to prefer one over the other.\n",
    "\n",
    "Once multiple LFs have been created, users can look at data points that have received no labels so far (or many conflicting labels) to get ideas for new LFs to write.\n",
    "Furthermore, if there are some classes that have votes from very few LFs, users might iterate by writing additional LFs to cover those parts of the dataset.\n",
    "**Note, however, that it is not necessary for LFs to assign labels to every data point;** in fact, most of the time your LFs will not have perfect dataset-wide coverage.\n",
    "We rely on the fact that the classifier that trains on labels from Snorkel has the power to _generalize_ and can therefore learn a good representation of the data even if the each data point in the training set does not receive a label from any LFs.\n",
    "In addition, note that **it is ok to have overlaps and conflicts between labeling functions**. For example, two good labeling functions can conflict when one of them is written to fix the errors of the other. We later use a _label model_ that learns how to combine the outputs of all (potentially conflicting) labeling functions to estimate label probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Look at examples for ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the process of writing LFs by looking at some examples in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>everyday I&amp;#39;m shufflin﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Charlie from LOST﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Very Nice !﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>CUTE  :)﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>I&amp;#39;m A SUBSCRIBER﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>😼👍😏 Like This Comment 😏👍😼﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Limit sun exposure while driving. Eliminate the hassle of having to swing  the car visor between the windshield and window.  https://www.kickstarter.com/projects/733634264/visortwin﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "128  everyday I&#39;m shufflin﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "151  Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "31   Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿   \n",
       "29   Charlie from LOST﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "237  Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "207  Very Nice !﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "406  CUTE  :)﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "325  I&#39;m A SUBSCRIBER﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "291  😼👍😏 Like This Comment 😏👍😼﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "252  Limit sun exposure while driving. Eliminate the hassle of having to swing  the car visor between the windshield and window.  https://www.kickstarter.com/projects/733634264/visortwin﻿                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "     label  \n",
       "128  0      \n",
       "151  1      \n",
       "31   1      \n",
       "29   0      \n",
       "237  0      \n",
       "207  0      \n",
       "406  0      \n",
       "325  1      \n",
       "291  1      \n",
       "252  1      "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display just the text and label\n",
    "df_dev[[\"text\", \"label\"]].sample(10, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Writing an LF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions in Snorkel are created with the `@labeling_function()` decorator, which wraps a function for evaluating on a single data point (in this case, a row of the DataFrame).\n",
    "\n",
    "Looking at samples of our data, we see multiple messages where spammers are trying to get viewers to look at \"my channel\" or \"my video,\" so we write a simple LF that labels an example as `SPAM` if it includes the word \"my\" and otherwise abstains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf import labeling_function\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "lfs = [keyword_my]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more LFs that we've written to a collection of data points, we use an `LFApplier`.\n",
    "\n",
    "Because our data points are represented with a Pandas DataFrame in this tutorial, we use the `PandasLFApplier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1586/1586 [00:00<00:00, 53067.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.apply import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `apply()` method is a label matrix which we generally refer to as `L` (or `L_[split name]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       ...,\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check performance on dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate the coverage of this LF by hand (i.e., the percentage of the dataset that it labels) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 19.9%\n"
     ]
    }
   ],
   "source": [
    "coverage = (L_train != ABSTAIN).mean()\n",
    "print(f\"Coverage: {coverage * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an estimate of its accuracy, we can label the development set with it and compare that to the few gold labels we do have.\n",
    "\n",
    "Note that we don't want to penalize the LF for examples where it abstained, so we calculate the accuracy only over those examples where the LF output a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 100/100 [00:00<00:00, 33662.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "L_dev = applier.apply(df_dev)\n",
    "L_dev_array = L_dev.squeeze()\n",
    "\n",
    "correct = L_dev_array == Y_dev\n",
    "labeled = L_dev_array != ABSTAIN\n",
    "accuracy = (correct * labeled).sum() / labeled.sum()\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the provided `metric_score()` helper method, which allows you to specify a metric to calculate and certain classes to ignore (such as ABSTAIN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.4 %\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis.metrics import metric_score\n",
    "\n",
    "# Calculate accuracy, ignore all examples for which the predicted label is ABSTAIN\n",
    "accuracy = metric_score(\n",
    "    golds=Y_dev, preds=L_dev_array, metric=\"accuracy\", filter_dict={\"preds\": [ABSTAIN]}\n",
    ")\n",
    "print(f\"Accuracy: {accuracy * 100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the helper class `LFAnalysis()` to report the following summary statistics for multiple LFs at once:\n",
    "* **Polarity**: The set of labels this LF outputs\n",
    "* **Coverage**: The fraction of the dataset the LF labels\n",
    "* **Overlaps**: The fraction of the dataset where this LF and at least one other LF label\n",
    "* **Conflicts**: The fraction of the dataset where this LF and at least one other LF label and disagree\n",
    "* **Correct**: The number of data points this LF labels correctly (if gold labels are provided)\n",
    "* **Incorrect**: The number of data points this LF labels incorrectly (if gold labels are provided)\n",
    "* **Emp. Acc.**: The empirical accuracy of this LF (if gold labels are provided)\n",
    "\n",
    "Since we only have one LF, overlaps and conflicts will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            j Polarity  Coverage  Overlaps  Conflicts  Correct  Incorrect  \\\n",
       "keyword_my  0  [1]      0.22      0.0       0.0        19       3           \n",
       "\n",
       "            Emp. Acc.  \n",
       "keyword_my  0.863636   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.analysis import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Balance accuracy/coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, by looking at the examples that an LF does and doesn't label, we can get ideas for how to improve it.\n",
    "\n",
    "The helper method `error_buckets()` groups examples by their predicted label and true label. For example, `buckets[(SPAM, HAM)]` contains the indices of data points that the LF labeled `SPAM` that actually belong to class `HAM`. This may give ideas for where the LF could be made more specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>\"....because I AM a champion...and you're gonna hear me roar!\"   Today I AM my own champion  Today I AM a champion for the Creator  Today I AM doing positive in my world Today I AM blessing and healing all around me Today I AM successful and  creating success  ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>I guss this song is one of my worst fears in life, to be with someone who abusive towered me and live with him.... ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>sorry to all my haters&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;for party rock en﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                       text  \\\n",
       "326  \"....because I AM a champion...and you're gonna hear me roar!\"   Today I AM my own champion  Today I AM a champion for the Creator  Today I AM doing positive in my world Today I AM blessing and healing all around me Today I AM successful and  creating success  ﻿   \n",
       "61   I guss this song is one of my worst fears in life, to be with someone who abusive towered me and live with him.... ﻿                                                                                                                                                     \n",
       "284  sorry to all my haters<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />for party rock en﻿                                                                                                                                                   \n",
       "\n",
       "     label  \n",
       "326  0      \n",
       "61   0      \n",
       "284  0      "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.analysis.error_analysis import error_buckets\n",
    "\n",
    "buckets = error_buckets(golds=Y_dev, preds=L_dev_array)\n",
    "\n",
    "df_dev[[\"text\", \"label\"]].iloc[buckets[(SPAM, HAM)]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, `buckets[(SPAM, SPAM)]` points to `SPAM` data points that the LF labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Suscribe My Channel Please XD lol﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG   SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>subscribe to my channel yo - DJ Feelz﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "367  Lol check out my chanell and subscribe please i want 5000 subs thats it im nearly their now﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "31   Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿   \n",
       "263  Suscribe My Channel Please XD lol﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "204  SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG   SnEakiESTG Good Music. Hood Muzik Subscribe 2 My Channel. Thanks For The Support. SnEakiESTG                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "100  subscribe to my channel yo - DJ Feelz﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "     label  \n",
       "367  1      \n",
       "31   1      \n",
       "263  1      \n",
       "204  1      \n",
       "100  1      "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[[\"text\", \"label\"]].iloc[buckets[(SPAM, SPAM)]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `buckets[(ABSTAIN, SPAM)]` points to data points that the LF abstained on that are actually `SPAM`.\n",
    "Many of these will be best captured by a separate LF, but browsing these examples can be a good check that your LF is capturing most of the examples that you intended it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Subscribe ME!﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Aslamu Lykum... From Pakistan﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>pleas subscribe on me for ps4 games video &lt;br /&gt;&lt;i&gt;______________________________&lt;/i&gt;&lt;br /&gt;if you have som tips so contact me on kik or skype&lt;br /&gt;&lt;i&gt;______________________________&lt;/i&gt;&lt;br /&gt;Kik: pander26&lt;br /&gt;Skype: sander.nicolaysen2&lt;br /&gt;&lt;i&gt;______________________&lt;/i&gt;&lt;br /&gt;pleas subscribe on me and Kashoo Gaming﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>http://www.gofundme.com/gvr7xg﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Limit sun exposure while driving. Eliminate the hassle of having to swing  the car visor between the windshield and window.  https://www.kickstarter.com/projects/733634264/visortwin﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                            text  \\\n",
       "35   Subscribe ME!﻿                                                                                                                                                                                                                                                                                                                \n",
       "313  Aslamu Lykum... From Pakistan﻿                                                                                                                                                                                                                                                                                                \n",
       "321  pleas subscribe on me for ps4 games video <br /><i>______________________________</i><br />if you have som tips so contact me on kik or skype<br /><i>______________________________</i><br />Kik: pander26<br />Skype: sander.nicolaysen2<br /><i>______________________</i><br />pleas subscribe on me and Kashoo Gaming﻿   \n",
       "42   http://www.gofundme.com/gvr7xg﻿                                                                                                                                                                                                                                                                                               \n",
       "252  Limit sun exposure while driving. Eliminate the hassle of having to swing  the car visor between the windshield and window.  https://www.kickstarter.com/projects/733634264/visortwin﻿                                                                                                                                        \n",
       "\n",
       "     label  \n",
       "35   1      \n",
       "313  1      \n",
       "321  1      \n",
       "42   1      \n",
       "252  1      "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[[\"text\", \"label\"]].iloc[buckets[(ABSTAIN, SPAM)]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all these examples, we notice that much of the time when \"my\" is used, it's referring to \"my channel\". We can update our LF to see how making this change affects accuracy and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 100/100 [00:00<00:00, 32091.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keywords_my_channel</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "keywords_my_channel  0  [1]      0.06      0.0       0.0        6         \n",
       "\n",
       "                     Incorrect  Emp. Acc.  \n",
       "keywords_my_channel  0          1.0        "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@labeling_function()\n",
    "def keywords_my_channel(x):\n",
    "    return SPAM if \"my channel\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "lfs = [keywords_my_channel]\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_dev = applier.apply(df_dev)\n",
    "\n",
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, accuracy does improve a bit, but it was already fairly accurate to begin with, and \"tightening\" the LF like this causes the coverage drops significantly, so we'll stick with the original LF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a single LF had high enough coverage to label our entire test dataset accurately, then we wouldn't need a classifier at all; we could just use that single simple heuristic to complete the task. But most problems are not that simple. Instead, we usually need to **combine multiple LFs** to label our dataset, both to increase the size of the generated training set (since we can't generate training labels for data points that all LFs abstained on) and to improve the overall accuracy of the training labels we generate by factoring in multiple different signals.\n",
    "\n",
    "In the following subsections, we'll show just a few of the many types of LFs that you could write to generate a training dataset for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Keyword LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text applications, some of the simplest LFs to write are often just keyword lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def keyword_my(x):\n",
    "    \"\"\"Spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_subscribe(x):\n",
    "    \"\"\"Spam comments ask users to subscribe to their channels.\"\"\"\n",
    "    return SPAM if \"subscribe\" in x.text else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_link(x):\n",
    "    \"\"\"Spam comments post links to other channels.\"\"\"\n",
    "    return SPAM if \"http\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_please(x):\n",
    "    \"\"\"Spam comments make requests rather than commenting.\"\"\"\n",
    "    return (\n",
    "        SPAM if any([word in x.text.lower() for word in [\"please\", \"plz\"]]) else ABSTAIN\n",
    "    )\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_song(x):\n",
    "    \"\"\"Ham comments actually talk about the video's content.\"\"\"\n",
    "    return HAM if \"song\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Pattern-matching LFs (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a little more control over a keyword search, we can look for regular expressions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii.  Heuristic LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may other heuristics or \"rules of thumb\" that you come up with as you look at the data.\n",
    "So long as you can express it in a function, it's a viable LF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LFs rely on fields that aren't present in the raw data, but can be derived from it.\n",
    "We can enrich our data (providing more fields for the LFs to refer to) using `Preprocessor`s.\n",
    "\n",
    "For example, we can use the fantastic NLP tool [spaCy](https://spacy.io/) to add lemmas, part-of-speech (pos) tags, etc. to each token.\n",
    "Snorkel provides a prebuilt preprocessor for spaCy called `SpacyPreprocessor` which adds a new field to the\n",
    "data point containing a [spaCy `Doc` object](https://spacy.io/api/doc).\n",
    "For more info, see the [`SpacyPreprocessor` documentation](https://snorkel.readthedocs.io/en/master/source/snorkel.labeling.preprocess.html#snorkel.labeling.preprocess.nlp.SpacyPreprocessor).\n",
    "\n",
    "\n",
    "If you prefer to use a different NLP tool, you can also wrap that as a `Preprocessor` and use it in the same way.\n",
    "For more info, see the [`preprocessor` documentation](https://snorkel.readthedocs.io/en/master/source/snorkel.labeling.preprocess.html#snorkel.labeling.preprocess.core.preprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages (2.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy english model\n",
    "# If you see an error in the next cell, restart the kernel\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "\n",
    "@labeling_function(preprocessors=[spacy])\n",
    "def has_person(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy is such a common preprocessor for NLP (Natural Language Processing) applications, we also provide an alias for a `labeling_function` that uses spaCy. This resulting LF is identical to the one defined manually above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf.nlp import nlp_labeling_function\n",
    "\n",
    "\n",
    "@nlp_labeling_function()\n",
    "def has_person_nlp(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if len(x.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in x.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding new domain-specific preprocessors and LF types is a great way to contribute to Snorkel!\n",
    "If you have an idea, feel free to reach out to the maintainers or submit a PR!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Third-party Model LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize other models, including ones trained for other tasks that are related to, but not the same as, the one we care about.\n",
    "\n",
    "For example, the [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) tool provides a pretrained sentiment analyzer. Our spam classification task is not the same as sentiment classification, but it turns out that `SPAM` and `HAM` comments have different distributions of sentiment scores, with `HAM` having more positive/subjective sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "spam_polarities = [\n",
    "    TextBlob(x.text).sentiment.polarity for i, x in df_dev.iterrows() if x.label == SPAM\n",
    "]\n",
    "ham_polarities = [\n",
    "    TextBlob(x.text).sentiment.polarity for i, x in df_dev.iterrows() if x.label == HAM\n",
    "]\n",
    "\n",
    "plt.hist([spam_polarities, ham_polarities])\n",
    "plt.title(\"Histogram of sentiment polarity scores for SPAM and HAM\")\n",
    "plt.xlabel(\"Sentiment polarity score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend([\"SPAM\", \"HAM\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def text_blob_sentiment(x):\n",
    "    x.sentiment = TextBlob(x.text).sentiment\n",
    "    return x\n",
    "\n",
    "\n",
    "@labeling_function(preprocessors=[text_blob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return HAM if x.sentiment.polarity > 0.3 else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(preprocessors=[text_blob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return HAM if x.sentiment.subjectivity > 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates just a handful of the types of LFs that one might write for this task.\n",
    "Many of these are no doubt suboptimal.\n",
    "The strength of this approach, however, is that the LF abstraction provides a flexible interface for conveying a huge variety of supervision signals, and the `LabelModel` is able to denoise these signals, reducing the need for painstaking manual fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    keyword_my,\n",
    "    lf_subscribe,\n",
    "    lf_link,\n",
    "    lf_please,\n",
    "    lf_song,\n",
    "    regex_check_out,\n",
    "    short_comment,\n",
    "    has_person_nlp,\n",
    "    textblob_polarity,\n",
    "    textblob_subjectivity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our full set of LFs, we can now apply these once again with `LFApplier` to get our the label matrices for the `train` and `dev` splits.\n",
    "We'll use the `train` split's label matrix to generate training labels with the Label Model.\n",
    "The `dev` split's label model is primarily helpful for looking at summary statistics.\n",
    "\n",
    "The Pandas format provides an easy interface that many practioners are familiar with, but it is also less optimized for scale.\n",
    "For larger datasets, more compute-intensive LFs, or larger LF sets, you may decide to use one of the other data formats\n",
    "that Snorkel supports natively, such as Dask DataFrames or PySpark DataFrames, and their corresponding applier objects.\n",
    "For more info, check out the [Snorkel API documentation](https://snorkel.readthedocs.io/en/master/source/snorkel.labeling.apply.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 12/1586 [00:00<00:13, 115.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 22/1586 [00:00<00:14, 108.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 32/1586 [00:00<00:14, 104.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 41/1586 [00:00<00:15, 98.88it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 52/1586 [00:00<00:15, 99.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 63/1586 [00:00<00:15, 101.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 73/1586 [00:00<00:15, 100.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 85/1586 [00:00<00:14, 101.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 97/1586 [00:00<00:14, 103.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 107/1586 [00:01<00:14, 99.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 118/1586 [00:01<00:14, 100.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 129/1586 [00:01<00:14, 101.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 140/1586 [00:01<00:14, 102.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 151/1586 [00:01<00:13, 104.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 162/1586 [00:01<00:13, 105.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 173/1586 [00:01<00:13, 106.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 184/1586 [00:01<00:15, 92.76it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 195/1586 [00:01<00:14, 95.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 206/1586 [00:02<00:14, 97.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▎        | 217/1586 [00:02<00:13, 100.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 228/1586 [00:02<00:13, 102.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 240/1586 [00:02<00:12, 105.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 251/1586 [00:02<00:13, 102.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 262/1586 [00:02<00:12, 103.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 273/1586 [00:02<00:12, 103.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 284/1586 [00:02<00:12, 103.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 295/1586 [00:02<00:12, 103.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 306/1586 [00:03<00:12, 99.17it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 318/1586 [00:03<00:12, 103.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 329/1586 [00:03<00:12, 103.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 340/1586 [00:03<00:12, 103.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 352/1586 [00:03<00:11, 106.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 363/1586 [00:03<00:11, 104.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▎       | 374/1586 [00:03<00:12, 99.78it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 385/1586 [00:03<00:11, 101.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▍       | 396/1586 [00:03<00:12, 97.21it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 406/1586 [00:04<00:12, 95.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 416/1586 [00:04<00:12, 96.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 428/1586 [00:04<00:11, 101.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 439/1586 [00:04<00:11, 98.68it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 450/1586 [00:04<00:11, 100.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 461/1586 [00:04<00:11, 98.51it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 472/1586 [00:04<00:11, 101.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 483/1586 [00:04<00:11, 94.21it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 494/1586 [00:04<00:11, 96.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 504/1586 [00:05<00:11, 94.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 514/1586 [00:05<00:11, 96.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 525/1586 [00:05<00:10, 97.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▎      | 535/1586 [00:05<00:10, 96.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 545/1586 [00:05<00:10, 95.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 556/1586 [00:05<00:10, 98.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 566/1586 [00:05<00:10, 98.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 577/1586 [00:05<00:10, 100.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 588/1586 [00:05<00:09, 100.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 599/1586 [00:05<00:09, 101.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 611/1586 [00:06<00:09, 102.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 622/1586 [00:06<00:09, 103.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███▉      | 633/1586 [00:06<00:09, 103.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 644/1586 [00:06<00:08, 104.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 655/1586 [00:06<00:09, 100.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 666/1586 [00:06<00:09, 101.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 677/1586 [00:06<00:09, 100.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 688/1586 [00:06<00:08, 101.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 699/1586 [00:06<00:08, 100.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 711/1586 [00:07<00:08, 104.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 722/1586 [00:07<00:08, 105.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▋     | 735/1586 [00:07<00:07, 110.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 747/1586 [00:07<00:07, 106.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 759/1586 [00:07<00:07, 110.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▊     | 772/1586 [00:07<00:07, 113.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 785/1586 [00:07<00:06, 116.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 797/1586 [00:07<00:08, 90.71it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 808/1586 [00:07<00:08, 95.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 821/1586 [00:08<00:07, 101.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 833/1586 [00:08<00:07, 106.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 845/1586 [00:08<00:06, 109.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 857/1586 [00:08<00:06, 111.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 869/1586 [00:08<00:06, 112.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 882/1586 [00:08<00:06, 114.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▋    | 894/1586 [00:08<00:06, 113.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 906/1586 [00:08<00:05, 115.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 918/1586 [00:08<00:05, 114.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 931/1586 [00:09<00:05, 117.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 943/1586 [00:09<00:05, 107.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 955/1586 [00:09<00:05, 109.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 967/1586 [00:09<00:05, 109.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 979/1586 [00:09<00:05, 108.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 991/1586 [00:09<00:05, 110.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 1004/1586 [00:09<00:05, 113.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 1016/1586 [00:09<00:05, 113.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▍   | 1028/1586 [00:09<00:04, 114.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 1040/1586 [00:10<00:04, 113.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▋   | 1052/1586 [00:10<00:04, 110.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 1065/1586 [00:10<00:04, 113.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 1077/1586 [00:10<00:04, 113.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 1089/1586 [00:10<00:04, 111.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 1101/1586 [00:10<00:04, 112.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 1113/1586 [00:10<00:04, 106.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 1125/1586 [00:10<00:04, 107.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 1138/1586 [00:10<00:04, 111.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 1150/1586 [00:11<00:03, 109.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 1161/1586 [00:11<00:03, 107.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 1172/1586 [00:11<00:03, 106.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 1183/1586 [00:11<00:03, 105.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 1194/1586 [00:11<00:03, 104.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 1205/1586 [00:11<00:03, 100.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 1217/1586 [00:11<00:03, 104.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 1228/1586 [00:11<00:03, 105.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 1239/1586 [00:11<00:03, 100.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 1250/1586 [00:12<00:03, 101.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████▉  | 1261/1586 [00:12<00:03, 97.84it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 1273/1586 [00:12<00:03, 101.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 1284/1586 [00:12<00:02, 102.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1295/1586 [00:12<00:02, 98.64it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1305/1586 [00:12<00:02, 93.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 1316/1586 [00:12<00:02, 95.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▎ | 1328/1586 [00:12<00:02, 98.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 1340/1586 [00:12<00:02, 102.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 1351/1586 [00:13<00:02, 103.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 1362/1586 [00:13<00:02, 102.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 1373/1586 [00:13<00:02, 104.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 1384/1586 [00:13<00:01, 103.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 1395/1586 [00:13<00:01, 102.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 1407/1586 [00:13<00:01, 105.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 1419/1586 [00:13<00:01, 107.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 1430/1586 [00:13<00:01, 102.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 1441/1586 [00:13<00:01, 100.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 1452/1586 [00:14<00:01, 96.95it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 1462/1586 [00:14<00:01, 95.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 1473/1586 [00:14<00:01, 99.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▎| 1484/1586 [00:14<00:01, 99.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 1495/1586 [00:14<00:00, 93.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 1507/1586 [00:14<00:00, 98.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 1517/1586 [00:14<00:00, 97.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▋| 1527/1586 [00:14<00:00, 97.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 1540/1586 [00:14<00:00, 102.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 1552/1586 [00:15<00:00, 102.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▊| 1563/1586 [00:15<00:00, 96.89it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 1575/1586 [00:15<00:00, 101.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1586/1586 [00:15<00:00, 100.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [00:00<00:00, 110.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [00:00<00:00, 102.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [00:00<00:00, 103.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 44/100 [00:00<00:00, 106.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 54/100 [00:00<00:00, 103.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 64/100 [00:00<00:00, 102.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 74/100 [00:00<00:00, 99.55it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 85/100 [00:00<00:00, 101.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 96/100 [00:00<00:00, 102.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 100/100 [00:00<00:00, 101.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 9/120 [00:00<00:01, 82.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 18/120 [00:00<00:01, 84.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 28/120 [00:00<00:01, 88.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 37/120 [00:00<00:00, 85.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 47/120 [00:00<00:00, 87.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 57/120 [00:00<00:00, 90.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 68/120 [00:00<00:00, 94.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 79/120 [00:00<00:00, 95.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 90/120 [00:00<00:00, 99.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 102/120 [00:01<00:00, 103.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 113/120 [00:01<00:00, 105.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 120/120 [00:01<00:00, 98.24it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_link</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_person_nlp</th>\n",
       "      <td>7</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.11</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "keyword_my             0  [1]      0.22      0.21      0.13       19        \n",
       "lf_subscribe           1  [1]      0.08      0.05      0.02       8         \n",
       "lf_link                2  [1]      0.10      0.07      0.05       10        \n",
       "lf_please              3  [1]      0.10      0.09      0.05       10        \n",
       "lf_song                4  [0]      0.16      0.11      0.06       11        \n",
       "regex_check_out        5  [1]      0.29      0.22      0.14       29        \n",
       "short_comment          6  [0]      0.28      0.17      0.05       19        \n",
       "has_person_nlp         7  [0]      0.15      0.12      0.04       10        \n",
       "textblob_polarity      8  [0]      0.28      0.26      0.11       18        \n",
       "textblob_subjectivity  9  [0]      0.08      0.08      0.04       5         \n",
       "\n",
       "                       Incorrect  Emp. Acc.  \n",
       "keyword_my             3          0.863636   \n",
       "lf_subscribe           0          1.000000   \n",
       "lf_link                0          1.000000   \n",
       "lf_please              0          1.000000   \n",
       "lf_song                5          0.687500   \n",
       "regex_check_out        0          1.000000   \n",
       "short_comment          9          0.678571   \n",
       "has_person_nlp         5          0.666667   \n",
       "textblob_polarity      10         0.642857   \n",
       "textblob_subjectivity  3          0.625000   "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)\n",
    "L_valid = applier.apply(df=df_valid)\n",
    "\n",
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our labeling functions vary in coverage, accuracy, and how much they overlap/conflict with one another.\n",
    "We can view a histogram of how many weak labels the data points in our dev set have to get an idea of our total coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that over half of our training dataset data points have 0 or 1 weak labels.\n",
    "Fortunately, the signal we do have can be used to train a classifier with a larger feature set than just these labeling functions that we've created, allowing it to generalize beyond what we've specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining Weak Labels with the Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to convert these many weak labels into a single _noise-aware_ probabilistic (or confidence-weighted) label per data point.\n",
    "A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "Y_pred_train = majority_model.predict(L=L_train)\n",
    "Y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we can clearly see by looking the summary statistics of our LFs in the previous section, they are not all equally accurate, and should ideally not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. To handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine our weak labels.\n",
    "\n",
    "This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task. For more technical details of this overall approach, see our [NeurIPS 2016](https://arxiv.org/abs/1605.07723) and [AAAI 2019](https://arxiv.org/abs/1810.02840) papers. For more info on the API, see the [`LabelModel` documentation](https://snorkel.readthedocs.io/en/master/source/snorkel.labeling.model.html#snorkel.labeling.model.label_model.LabelModel).\n",
    "\n",
    "Note that no gold labels are used during the training process.\n",
    "The `LabelModel` is able to learn weights for the labeling functions using only the label matrix as input.\n",
    "We also specify the `cardinality`, or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[0 epochs]: TRAIN:[loss=0.129]\n",
      "[50 epochs]: TRAIN:[loss=0.013]\n",
      "[100 epochs]: TRAIN:[loss=0.010]\n",
      "[150 epochs]: TRAIN:[loss=0.009]\n",
      "[200 epochs]: TRAIN:[loss=0.009]\n",
      "[250 epochs]: TRAIN:[loss=0.008]\n",
      "[300 epochs]: TRAIN:[loss=0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350 epochs]: TRAIN:[loss=0.008]\n",
      "[400 epochs]: TRAIN:[loss=0.008]\n",
      "[450 epochs]: TRAIN:[loss=0.008]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=50, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   85.0%\n",
      "Label Model Accuracy:     88.3%\n"
     ]
    }
   ],
   "source": [
    "majority_acc = majority_model.score(L=L_valid, Y=Y_valid)[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_valid, Y=Y_valid)[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our `LabelModel` improves over the majority vote baseline!\n",
    "However, it is typically **not suitable as an inference-time model** to make predictions for unseen examples, due to (among other things) some data points having all abstain labels.\n",
    "In the next section, we will use the output of the label model as  training labels to train a\n",
    "discriminative classifier to see if we can improve performance further.\n",
    "This classifier will only need the text of the comment to make predictions, making it much more suitable\n",
    "for inference over unseen comments.\n",
    "For more information on the properties of the label model and when to use it, see the [Snorkel guides]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run error analysis after the label model has been trained.\n",
    "For example, let's take a look at false positives from the dev set, which might inspire some more LFs that vote `SPAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>I&amp;#39;m A SUBSCRIBER﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Check out Em&amp;#39;s dope new song monster here: /watch?v=w6gkM-XNY2M  MMLP2 FTW :)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>You guys should check out this EXTRAORDINARY website called ZONEPA.COM .   You can make money online and start working from home today as I am!   I am making over $3,000+ per month at ZONEPA.COM !   Visit Zonepa.com and check it out!  Why does the answer rehabilitate the blushing limit? The push depreciateds the steel. How does the beautiful selection edit the range?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Aslamu Lykum... From Pakistan﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Subscribe ME!﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "325  I&#39;m A SUBSCRIBER﻿                                                                                                                                                                                                                                                                                                                                                               \n",
       "334  Check out Em&#39;s dope new song monster here: /watch?v=w6gkM-XNY2M  MMLP2 FTW :)                                                                                                                                                                                                                                                                                                   \n",
       "159  You guys should check out this EXTRAORDINARY website called ZONEPA.COM .   You can make money online and start working from home today as I am!   I am making over $3,000+ per month at ZONEPA.COM !   Visit Zonepa.com and check it out!  Why does the answer rehabilitate the blushing limit? The push depreciateds the steel. How does the beautiful selection edit the range?   \n",
       "313  Aslamu Lykum... From Pakistan﻿                                                                                                                                                                                                                                                                                                                                                      \n",
       "35   Subscribe ME!﻿                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "     label  probability  \n",
       "325  1      0.0          \n",
       "334  1      0.0          \n",
       "159  1      0.0          \n",
       "313  1      0.0          \n",
       "35   1      0.0          "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev_prob = majority_model.predict_proba(L=L_dev)\n",
    "Y_dev_pred = Y_dev_prob >= 0.5\n",
    "buckets = error_buckets(golds=Y_dev, preds=Y_dev_pred[:, 1])\n",
    "\n",
    "df_dev_fp = df_dev[[\"text\", \"label\"]].iloc[buckets[(HAM, SPAM)]]\n",
    "df_dev_fp[\"probability\"] = Y_dev_prob[buckets[(HAM, SPAM)], 1]\n",
    "\n",
    "df_dev_fp.sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly confirm that the labels the `LabelModel` produces are probabilistic in nature.\n",
    "The following histogram shows the confidences we have that each data point has the label SPAM.\n",
    "The points we are least certain about will have labels close to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_probabilities_histogram(Y):\n",
    "    plt.hist(Y, bins=10)\n",
    "    plt.xlabel(\"Probability of SPAM\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "Y_probs_train = label_model.predict_proba(L=L_train)\n",
    "plot_probabilities_histogram(Y_probs_train[:, SPAM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last section to train a classifier for our task.\n",
    "\n",
    "Note that because the output of the Snorkel `LabelModel` is just a set of labels, Snorkel easily integrates with most popular libraries for performing supervised learning: TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost, etc.\n",
    "\n",
    "In this tutorial we demonstrate using classifiers from Keras and Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "words_train = [row.text for i, row in df_train.iterrows()]\n",
    "words_dev = [row.text for i, row in df_dev.iterrows()]\n",
    "words_valid = [row.text for i, row in df_valid.iterrows()]\n",
    "words_test = [row.text for i, row in df_test.iterrows()]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(words_train)\n",
    "X_dev = vectorizer.transform(words_dev)\n",
    "X_valid = vectorizer.transform(words_valid)\n",
    "X_test = vectorizer.transform(words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Unlabeled Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, some of the data points in our training set received no weak labels from our LFs.\n",
    "These examples convey no supervision signal and tend to hurt performance, so we filter them out before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = L_train.sum(axis=1) != ABSTAIN * len(lfs)\n",
    "X_train = X_train[mask, :]\n",
    "Y_probs_train = Y_probs_train[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Classifier with Probabilistic Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Keras, a popular high-level API for building models in TensorFlow, to build a simple logistic regression classifier.\n",
    "We compile it with a `categorical_crossentropy` loss so that it can handle probabilistic labels instead of integer labels.\n",
    "Using a _noise-aware loss_ &mdash; one that uses probabilistic labels &mdash; for our discriminative model lets\n",
    "us take full advantage of the label model's learning procedure (see our [NeurIPS 2016 paper](https://arxiv.org/abs/1605.07723)).\n",
    "We use the common settings of an `Adam` optimizer and early stopping (evaluating the model on the validation set after each epoch and reloading the weights from when it achieved the best score).\n",
    "For more information on Keras, see the [Keras documentation](https://keras.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 21:56:44.165285 140483671148352 deprecation.py:506] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "Test Accuracy: 0.924\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis.utils import probs_to_preds, preds_to_probs\n",
    "from snorkel.analysis.metrics import metric_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Our model is a simple linear layer mapping from feature\n",
    "# vectors to the number of labels in our problem (2).\n",
    "keras_model = tf.keras.Sequential()\n",
    "keras_model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=2,\n",
    "        input_dim=X_train.shape[1],\n",
    "        activation=tf.nn.softmax,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "keras_model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", patience=10, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "keras_model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_probs_train,\n",
    "    validation_data=(X_valid, preds_to_probs(Y_valid, 2)),\n",
    "    callbacks=[early_stopping],\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "Y_probs_test = keras_model.predict(x=X_test)\n",
    "Y_preds_test = probs_to_preds(probs=Y_probs_test)\n",
    "print(\n",
    "    f\"Test Accuracy: {metric_score(golds=Y_test, preds=Y_preds_test, metric='accuracy')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe an additional boost in accuracy over the `LabelModel` by multiple points!\n",
    "By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model,\n",
    "we were able to generalize beyond the noisy labeling heuristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this to the score we could have gotten if we had used our small labeled dev set directly as training data instead of using it to guide the creation of LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 21:56:46.454246 140483671148352 deprecation.py:323] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "keras_rounded_model = tf.keras.Sequential()\n",
    "keras_rounded_model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        input_dim=X_train.shape[1],\n",
    "        activation=tf.nn.sigmoid,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "keras_rounded_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "keras_rounded_model.fit(\n",
    "    x=X_dev,\n",
    "    y=Y_dev,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=[early_stopping],\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "Y_probs_test = keras_rounded_model.predict(x=X_test)\n",
    "Y_preds_test = np.round(Y_probs_test)\n",
    "print(\n",
    "    f\"Test Accuracy: {metric_score(golds=Y_test, preds=Y_preds_test, metric='accuracy')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn with Rounded Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use a library or model that doesn't accept probabilistic labels, we can replace each label distribution with the label of the class that has the maximum probability.\n",
    "This can easily be done using the helper method `probs_to_preds` (note, however, that this transformation is lossy, as we no longer have values for our confidence in each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds_train = probs_to_preds(probs=Y_probs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this allows us to use standard models from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(X=X_train, y=Y_preds_train)\n",
    "\n",
    "sklearn_model.score(X=X_test, y=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we accomplished the following:\n",
    "* We introduced the concept of Labeling Functions (LFs) and demonstrated some of the forms they can take.\n",
    "* We used the Snorkel `LabelModel` to automatically learn how to combine many weak labels into strong probabilistic labels.\n",
    "* We showed that a classifier trained on a weakly supervised dataset can outperform an approach based on the LFs alone as it learns to generalize beyond the noisy heuristics we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enjoyed this tutorial and you've already checked out the Snorkel 101 Guide, check out the [`snorkel-tutorials` table of contents](https://github.com/snorkel-team/snorkel-tutorials#snorkel-tutorials) for other tutorials that you may find interesting, including demonstrations of how to use Snorkel\n",
    "\n",
    "* As part of a [hybrid crowdsourcing pipeline](https://github.com/snorkel-team/snorkel-tutorials/tree/master/crowdsourcing)\n",
    "* For [scene-graph detection over images](https://github.com/snorkel-team/snorkel-tutorials/tree/master/scene_graph)\n",
    "* For [information extraction over text](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spouse)\n",
    "* For [data augmentation](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)\n",
    "\n",
    "and many more!\n",
    "You can also visit the [Snorkel homepage](http://snorkel.org) or [Snorkel API documentation](https://snorkel.readthedocs.io) for more info!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
