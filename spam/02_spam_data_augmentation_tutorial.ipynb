{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Transformation Functions Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the process of using Snorkel Transformation Functions (TFs) to classify YouTube comments as `SPAM` or `HAM` (not spam). For more details on the task, check out the main labeling functions [tutorial](https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/01_spam_tutorial.ipynb).\n",
    "For an overview of Snorkel, visit [snorkel.org](http://snorkel.org).\n",
    "You can also check out the [Snorkel API documentation](https://snorkel.readthedocs.io/).\n",
    "\n",
    "For our task, we have access to some labeled YouTube comments for training. We generate additional data by transforming the labeled comments using **_Transformation Functions_**.\n",
    "\n",
    "The tutorial is divided into four parts:\n",
    "1. **Loading Data**: We load a [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle.\n",
    "2. **Writing Transformation Functions**: We write Transformation Functions (TFs) that can be applied to training examples to generate new training examples.\n",
    "3. **Applying Transformation Functions**: We apply a sequence of TFs to each training data point, using a random policy, to generate an augmented training set.\n",
    "4. **Training A Model**: We use the augmented training set to train an LSTM model for classifying new comments as `SPAM` or `HAM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits in Snorkel\n",
    "\n",
    "We split our data into 3 sets:\n",
    "* **Training Set**: The largest split of the dataset. These are the examples used for training, and also the ones that transformation functions are applied on.\n",
    "* **Validation Set**: A labeled set used to tune hyperparameters and/or perform early stopping while training the classifier.\n",
    "* **Test Set**: A labeled set for final evaluation of our classifier. This set should only be used for final evaluation, _not_ tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Kaggle dataset and create Pandas DataFrame objects for each of the sets described above.\n",
    "The two main columns in the DataFrames are:\n",
    "* **`text`**: Raw text content of the comment\n",
    "* **`label`**: Whether the comment is `SPAM` (1) or `HAM` (0).\n",
    "\n",
    "For more details, check out the labeling functions [tutorial](https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/spam_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "# Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, _, df_valid, df_test = load_spam_dataset(load_train_labels=True)\n",
    "\n",
    "# We pull out the label vectors for ease of use later\n",
    "Y_valid = df_valid[\"label\"].values\n",
    "Y_train = df_train[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :D﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                 date  \\\n",
       "0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1      Salim Tayara  2014-11-02T14:33:30   \n",
       "2           Phuc Ly  2014-01-20T15:27:47   \n",
       "3      DropShotSk8r  2014-01-19T04:27:18   \n",
       "4            css403  2014-11-07T14:25:48   \n",
       "\n",
       "                                                text  label  video  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx...      1      1  \n",
       "1  if your like drones, plz subscribe to Kamal Ta...      1      1  \n",
       "2                     go here to check the views :3﻿      0      1  \n",
       "3            Came here to check the views, goodbye.﻿      0      1  \n",
       "4                      i am 2,126,492,636 viewer :D﻿      0      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Transformation Functions\n",
    "\n",
    "Transformation Functions are functions that can be applied to a training example to create another valid training example. For example, for image classification problems, it is common to rotate or crop images in the training data to create new training inputs. Transformation functions should be atomic e.g. a small rotation of an image, or changing a single word in a sentence. We then compose multiple transformation functions when applying them to training examples.\n",
    "\n",
    "Our task involves processing text. Some [common](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28) [ways](https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610) to augment text includes replacing words with their synonyms, or replacing names entities with other entities. Applying these operations to a comment shouldn't change whether it is `SPAM` or not.\n",
    "\n",
    "Transformation functions in Snorkel are created with the `@transformation_function()` decorator, which wraps a function that takes in a single data point and returns a transformed version of the data point. If no transformation is possible, the function should return `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding `pre` mappers.\n",
    "Some TFs rely on fields that aren't present in the raw data, but can be derived from it.\n",
    "We can enrich our data (providing more fields for the TFs to refer to) using map functions specified in the `pre` field of the transformation_function decorator (similar to `preprocessor` used for Labeling Functions).\n",
    "\n",
    "For example, we can use the fantastic NLP tool [spaCy](https://spacy.io/) to add lemmas, part-of-speech (pos) tags, etc. to each token.\n",
    "Snorkel provides a prebuilt preprocessor for spaCy called `SpacyPreprocessor` which adds a new field to the\n",
    "data point containing a [spaCy `Doc` object](https://spacy.io/api/doc). It uses memoization internally, so it will not reparse the text after applying each TF unless the text's hash changes.\n",
    "For more info, see the [`SpacyPreprocessor` documentation](https://snorkel.readthedocs.io/en/redux/packages/_autosummary/preprocess/snorkel.preprocess.nlp.SpacyPreprocessor.html#snorkel.preprocess.nlp.SpacyPreprocessor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages (2.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy english model\n",
    "# If you see an error in the next cell, restart the kernel\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "import numpy as np\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "# Replace a random named entity with a different entity of the same type.\n",
    "@transformation_function(pre=[spacy])\n",
    "def change_person(x):\n",
    "    person_names = [ent.text for ent in x.doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # If there is at least one person name, replace a random one. Else return None.\n",
    "    if person_names:\n",
    "        name_to_replace = np.random.choice(person_names)\n",
    "        replacement_name = np.random.choice(replacement_names)\n",
    "        x.text = x.text.replace(name_to_replace, replacement_name)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Swap two adjectives at random.\n",
    "@transformation_function(pre=[spacy])\n",
    "def swap_adjectives(x):\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    # Check that there are at least two adjectives to swap.\n",
    "    if len(adjective_idxs) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_idxs, 2, replace=False))\n",
    "        # Swap tokens in positions idx1 and idx2.\n",
    "        x.text = \" \".join(\n",
    "            [\n",
    "                x.doc[:idx1].text,\n",
    "                x.doc[idx2].text,\n",
    "                x.doc[1 + idx1 : idx2].text,\n",
    "                x.doc[idx1].text,\n",
    "                x.doc[1 + idx2 :].text,\n",
    "            ]\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add some transformation functions that use `wordnet` from [NLTK](https://www.nltk.org/) to replace different parts of speech with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    return \" \".join([spacy_doc[:idx].text, replacement, spacy_doc[1 + idx :].text])\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    if verb_idxs:\n",
    "        # Pick random verb idx to replace.\n",
    "        idx = np.random.choice(verb_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "        # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    # Get indices of noun tokens in sentence.\n",
    "    noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "    if noun_idxs:\n",
    "        # Pick random noun idx to replace.\n",
    "        idx = np.random.choice(noun_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "        # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    # Get indices of adjective tokens in sentence.\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adjective_idxs:\n",
    "        # Pick random adjective idx to replace.\n",
    "        idx = np.random.choice(adjective_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "        # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.text = replace_token(x.doc, idx, synonym)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try running the TFs on our training data to demonstrate their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Transformed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change_person</td>\n",
       "      <td>Check out Berzerk video on my channel ! :D</td>\n",
       "      <td>Check out Kaylee Peterson video on my channel ! :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap_adjectives</td>\n",
       "      <td>hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a young rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff</td>\n",
       "      <td>hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a more rapper and i love to do it and i just wanna share my music with young people  just click my picture and then see if you like my stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace_verb_with_synonym</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is use  titles of old rock songs for lyrics..﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace_noun_with_synonym</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿</td>\n",
       "      <td>\"eye of the tiger\" \"i am the champion\" seems like katy perry is using   title of old rock songs for lyrics..﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace_adjective_with_synonym</td>\n",
       "      <td>Haha its so funny to see the salt of westerners that top views of youtube  goes to video they dont even understand, keep the salt up!﻿</td>\n",
       "      <td>Haha its so amusing to see the salt of westerners that top views of youtube  goes to video they dont even understand, keep the salt up!﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TF Name  \\\n",
       "0  change_person                    \n",
       "1  swap_adjectives                  \n",
       "2  replace_verb_with_synonym        \n",
       "3  replace_noun_with_synonym        \n",
       "4  replace_adjective_with_synonym   \n",
       "\n",
       "                                                                                                                                                                                                                                                  Original Text  \\\n",
       "0  Check out Berzerk video on my channel ! :D                                                                                                                                                                                                                     \n",
       "1  hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a young rapper and i love to do it and i just wanna share my music with more people  just click my picture and then see if you like my stuff   \n",
       "2  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿                                                                                                                                                  \n",
       "3  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using  titles of old rock songs for lyrics..﻿                                                                                                                                                  \n",
       "4  Haha its so funny to see the salt of westerners that top views of youtube  goes to video they dont even understand, keep the salt up!﻿                                                                                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                               Transformed Text  \n",
       "0  Check out Kaylee Peterson video on my channel ! :D                                                                                                                                                                                                            \n",
       "1  hey guys look im aware im spamming and it pisses people off but please take a moment to check out my music.  im a more rapper and i love to do it and i just wanna share my music with young people  just click my picture and then see if you like my stuff  \n",
       "2  \"eye of the tiger\" \"i am the champion\" seems like katy perry is use  titles of old rock songs for lyrics..﻿                                                                                                                                                   \n",
       "3  \"eye of the tiger\" \"i am the champion\" seems like katy perry is using   title of old rock songs for lyrics..﻿                                                                                                                                                 \n",
       "4  Haha its so amusing to see the salt of westerners that top views of youtube  goes to video they dont even understand, keep the salt up!﻿                                                                                                                      "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Make augmentations deterministic.\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Prevent truncating displayed sentences.\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "tfs = [\n",
    "    change_person,\n",
    "    swap_adjectives,\n",
    "    replace_verb_with_synonym,\n",
    "    replace_noun_with_synonym,\n",
    "    replace_adjective_with_synonym,\n",
    "]\n",
    "\n",
    "transformed_examples = []\n",
    "for tf in tfs:\n",
    "    for i, row in df_train.sample(frac=1, random_state=2).iterrows():\n",
    "        transformed_or_none = tf(row)\n",
    "        # If TF returned a transformed example, record it in dict and move to next TF.\n",
    "        if transformed_or_none is not None:\n",
    "            transformed_examples.append(\n",
    "                OrderedDict(\n",
    "                    {\n",
    "                        \"TF Name\": tf.name,\n",
    "                        \"Original Text\": row.text,\n",
    "                        \"Transformed Text\": transformed_or_none.text,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "pd.DataFrame(transformed_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a couple of things about the TFs.\n",
    "* Sometimes they make trivial changes (_titles_ -> _title_ for replace_noun_with_synonym). This can still be helpful for training our model, because it teaches the model that singular and plural versions of words have similar meanings.\n",
    "* Sometimes they make the sentence less meaningful (e.g. swapping _young_ and _more_ for swap_adjectives).\n",
    "\n",
    "Data augmentation can be tricky for text inputs, so we expect most TFs to be a little flawed. But these TFs can  be useful despite the flaws; see [this paper](https://arxiv.org/pdf/1901.11196.pdf) for gains resulting from similar TFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying Transformation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more TFs that we've written to a collection of data points, we use a `TFApplier`.\n",
    "Because our data points are represented with a Pandas DataFrame in this tutorial, we use the `PandasTFApplier` class. In addition, we can apply multiple TFs in a sequence to each example. A `policy` is used to determine what sequence of TFs to apply to each example. In this case, we just use a `MeanFieldPolicy` that picks 2 TFs at random per example, with probabilities given by `p`. We give higher probabilities to the replace_X_with_synonym TFs, since those provide more information to the model. The `n_per_original` argument determines how many augmented examples to generate per original example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 9/1586 [00:00<00:19, 80.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 16/1586 [00:00<00:21, 73.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 23/1586 [00:00<00:22, 70.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 31/1586 [00:00<00:21, 72.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 38/1586 [00:00<00:21, 71.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 46/1586 [00:00<00:22, 69.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 55/1586 [00:00<00:20, 73.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 63/1586 [00:00<00:21, 71.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 70/1586 [00:00<00:21, 70.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 80/1586 [00:01<00:19, 75.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 88/1586 [00:01<00:19, 75.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 98/1586 [00:01<00:18, 79.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 107/1586 [00:01<00:20, 71.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 115/1586 [00:01<00:20, 72.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 123/1586 [00:01<00:21, 68.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 133/1586 [00:01<00:19, 74.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 141/1586 [00:01<00:20, 71.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 152/1586 [00:02<00:18, 78.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 161/1586 [00:02<00:18, 75.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 170/1586 [00:02<00:18, 77.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 178/1586 [00:02<00:22, 62.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 187/1586 [00:02<00:20, 67.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 196/1586 [00:02<00:19, 71.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 206/1586 [00:02<00:17, 77.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▎        | 215/1586 [00:02<00:17, 80.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 224/1586 [00:03<00:16, 82.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▍        | 233/1586 [00:03<00:16, 83.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 242/1586 [00:03<00:15, 84.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 251/1586 [00:03<00:16, 79.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 262/1586 [00:03<00:16, 80.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 271/1586 [00:03<00:16, 79.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 280/1586 [00:03<00:16, 77.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 288/1586 [00:03<00:18, 71.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 296/1586 [00:03<00:17, 72.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 305/1586 [00:04<00:17, 74.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 313/1586 [00:04<00:16, 74.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 323/1586 [00:04<00:15, 80.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 332/1586 [00:04<00:16, 75.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 342/1586 [00:04<00:15, 80.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 351/1586 [00:04<00:15, 77.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 360/1586 [00:04<00:15, 77.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 368/1586 [00:04<00:17, 70.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▎       | 376/1586 [00:05<00:21, 56.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 383/1586 [00:05<00:20, 59.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▍       | 391/1586 [00:05<00:18, 64.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 398/1586 [00:05<00:20, 56.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 406/1586 [00:05<00:19, 61.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 414/1586 [00:05<00:18, 63.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 423/1586 [00:05<00:16, 69.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 432/1586 [00:05<00:15, 72.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 440/1586 [00:06<00:15, 72.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 448/1586 [00:06<00:15, 73.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 456/1586 [00:06<00:15, 75.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 464/1586 [00:06<00:16, 69.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 473/1586 [00:06<00:15, 70.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 481/1586 [00:06<00:16, 66.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 488/1586 [00:06<00:16, 64.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 495/1586 [00:06<00:17, 62.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 503/1586 [00:06<00:16, 66.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 512/1586 [00:07<00:15, 70.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 520/1586 [00:07<00:16, 65.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 529/1586 [00:07<00:15, 69.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 537/1586 [00:07<00:14, 70.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 545/1586 [00:07<00:15, 68.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▍      | 552/1586 [00:07<00:15, 64.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 562/1586 [00:07<00:14, 71.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 570/1586 [00:07<00:14, 70.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 578/1586 [00:07<00:14, 71.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 586/1586 [00:08<00:14, 69.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 594/1586 [00:08<00:14, 69.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 602/1586 [00:08<00:14, 70.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 612/1586 [00:08<00:12, 75.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 621/1586 [00:08<00:12, 75.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███▉      | 629/1586 [00:08<00:13, 71.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 637/1586 [00:08<00:13, 72.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 647/1586 [00:08<00:12, 77.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 655/1586 [00:09<00:13, 70.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 664/1586 [00:09<00:12, 74.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 673/1586 [00:09<00:11, 76.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 681/1586 [00:09<00:12, 70.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▎     | 690/1586 [00:09<00:12, 74.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 698/1586 [00:09<00:12, 72.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 707/1586 [00:09<00:11, 77.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 717/1586 [00:09<00:10, 79.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 726/1586 [00:09<00:10, 82.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▋     | 737/1586 [00:10<00:09, 85.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 747/1586 [00:10<00:09, 87.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 758/1586 [00:10<00:09, 91.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 769/1586 [00:10<00:08, 93.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 782/1586 [00:10<00:08, 98.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 793/1586 [00:10<00:08, 88.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 803/1586 [00:10<00:08, 87.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████▏    | 814/1586 [00:10<00:08, 92.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 824/1586 [00:10<00:08, 93.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 835/1586 [00:11<00:07, 96.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 846/1586 [00:11<00:07, 100.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 858/1586 [00:11<00:07, 103.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 869/1586 [00:11<00:06, 105.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 880/1586 [00:11<00:07, 100.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 891/1586 [00:11<00:07, 95.93it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 901/1586 [00:11<00:07, 96.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 911/1586 [00:11<00:07, 93.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 921/1586 [00:11<00:07, 90.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 931/1586 [00:12<00:07, 89.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 943/1586 [00:12<00:06, 95.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 955/1586 [00:12<00:06, 100.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 966/1586 [00:12<00:06, 95.85it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 976/1586 [00:12<00:06, 87.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 986/1586 [00:12<00:06, 89.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 996/1586 [00:12<00:06, 91.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 1008/1586 [00:12<00:06, 95.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 1018/1586 [00:12<00:06, 93.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▍   | 1028/1586 [00:13<00:06, 92.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 1038/1586 [00:13<00:06, 86.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 1047/1586 [00:13<00:06, 86.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 1058/1586 [00:13<00:05, 91.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 1069/1586 [00:13<00:05, 96.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 1079/1586 [00:13<00:05, 87.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 1089/1586 [00:13<00:06, 79.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 1099/1586 [00:13<00:05, 82.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████▉   | 1109/1586 [00:14<00:05, 84.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 1118/1586 [00:14<00:05, 83.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 1128/1586 [00:14<00:05, 86.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 1138/1586 [00:14<00:04, 90.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 1148/1586 [00:14<00:06, 62.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 1156/1586 [00:14<00:08, 53.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 1163/1586 [00:15<00:09, 44.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 1171/1586 [00:15<00:08, 49.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 1177/1586 [00:15<00:09, 41.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 1183/1586 [00:15<00:09, 44.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 1189/1586 [00:15<00:09, 41.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 1194/1586 [00:15<00:10, 38.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 1199/1586 [00:15<00:10, 36.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 1204/1586 [00:16<00:10, 34.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▋  | 1210/1586 [00:16<00:09, 38.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 1216/1586 [00:16<00:09, 40.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 1224/1586 [00:16<00:07, 47.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 1230/1586 [00:16<00:07, 45.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 1235/1586 [00:16<00:09, 38.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 1240/1586 [00:16<00:09, 37.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 1245/1586 [00:17<00:08, 38.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 1251/1586 [00:17<00:08, 41.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 1256/1586 [00:17<00:08, 39.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████▉  | 1261/1586 [00:17<00:08, 36.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████▉  | 1266/1586 [00:17<00:08, 37.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 1271/1586 [00:17<00:07, 39.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 1276/1586 [00:17<00:08, 37.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 1280/1586 [00:17<00:08, 35.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 1286/1586 [00:18<00:07, 39.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 1291/1586 [00:18<00:08, 35.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1295/1586 [00:18<00:09, 30.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1299/1586 [00:18<00:09, 30.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1303/1586 [00:18<00:09, 30.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 1307/1586 [00:18<00:09, 29.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 1313/1586 [00:19<00:08, 32.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 1317/1586 [00:19<00:07, 34.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▎ | 1325/1586 [00:19<00:06, 40.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 1330/1586 [00:19<00:06, 38.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 1335/1586 [00:19<00:06, 37.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 1341/1586 [00:19<00:06, 39.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 1346/1586 [00:19<00:06, 38.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 1351/1586 [00:19<00:06, 37.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 1356/1586 [00:20<00:05, 39.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 1361/1586 [00:20<00:06, 37.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 1365/1586 [00:20<00:06, 35.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 1369/1586 [00:20<00:06, 34.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 1375/1586 [00:20<00:05, 35.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 1379/1586 [00:20<00:06, 32.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 1385/1586 [00:20<00:05, 37.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 1390/1586 [00:20<00:05, 35.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 1395/1586 [00:21<00:05, 37.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 1400/1586 [00:21<00:04, 39.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 1406/1586 [00:21<00:04, 40.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 1413/1586 [00:21<00:03, 45.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 1418/1586 [00:21<00:04, 37.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████▉ | 1423/1586 [00:21<00:04, 38.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 1428/1586 [00:21<00:04, 34.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 1433/1586 [00:22<00:04, 35.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 1437/1586 [00:22<00:04, 36.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 1441/1586 [00:22<00:04, 35.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 1445/1586 [00:22<00:05, 24.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████▏| 1450/1586 [00:22<00:04, 28.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 1455/1586 [00:22<00:04, 32.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 1459/1586 [00:23<00:04, 28.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 1466/1586 [00:23<00:03, 33.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 1471/1586 [00:23<00:03, 35.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 1476/1586 [00:23<00:03, 32.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 1480/1586 [00:23<00:03, 32.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 1487/1586 [00:23<00:02, 37.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 1492/1586 [00:23<00:03, 30.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 1498/1586 [00:24<00:02, 33.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▍| 1503/1586 [00:24<00:02, 37.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 1508/1586 [00:24<00:02, 38.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 1513/1586 [00:24<00:02, 34.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 1517/1586 [00:24<00:02, 28.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 1522/1586 [00:24<00:02, 31.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▋| 1528/1586 [00:24<00:01, 35.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 1532/1586 [00:24<00:01, 35.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 1537/1586 [00:25<00:01, 37.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 1541/1586 [00:25<00:01, 33.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 1545/1586 [00:25<00:01, 29.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 1549/1586 [00:25<00:01, 28.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 1553/1586 [00:25<00:01, 26.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 1556/1586 [00:25<00:01, 25.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 1560/1586 [00:26<00:01, 24.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▊| 1564/1586 [00:26<00:00, 27.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 1570/1586 [00:26<00:00, 31.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 1577/1586 [00:26<00:00, 37.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 1582/1586 [00:26<00:00, 39.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1586/1586 [00:26<00:00, 59.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import MeanFieldPolicy, PandasTFApplier\n",
    "\n",
    "policy = MeanFieldPolicy(\n",
    "    len(tfs),\n",
    "    sequence_length=2,\n",
    "    n_per_original=2,\n",
    "    keep_original=True,\n",
    "    p=[0.05, 0.05, 0.3, 0.3, 0.3],\n",
    ")\n",
    "tf_applier = PandasTFApplier(tfs, policy)\n",
    "df_train_augmented = tf_applier.apply(df_train)\n",
    "Y_train_augmented = df_train_augmented[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set size: 1586\n",
      "Augmented training set size: 2514\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training set size: {len(df_train)}\")\n",
    "print(f\"Augmented training set size: {len(df_train_augmented)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost doubled our dataset using TFs! Note that despite `n_per_original` being set to 2, our dataset may not exactly triple in size, because sometimes TFs return `None` instead of a new example (e.g. `change_person` when applied to a sentence with no persons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training A Model\n",
    "\n",
    "Our final step is to use the augmented data to train a model. We train an LSTM (Long Short Term Memory) model, which is a very standard architecture for text processing tasks.\n",
    "\n",
    "The next cell makes keras results reproducible. You can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
    ")\n",
    "\n",
    "tf.compat.v1.set_random_seed(1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add some boilerplate code for creating an LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(num_buckets, embed_dim=16, rnn_state_size=64):\n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    lstm_model.add(tf.keras.layers.Embedding(num_buckets, embed_dim))\n",
    "    lstm_model.add(tf.keras.layers.LSTM(rnn_state_size, activation=tf.nn.relu))\n",
    "    lstm_model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "    lstm_model.compile(\"Adagrad\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 01:47:14.690174 140401927157568 deprecation.py:506] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 01:47:14.706047 140401927157568 deprecation.py:506] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 01:47:14.894868 140401927157568 deprecation.py:323] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0808 01:47:15.372925 140401927157568 deprecation.py:506] From /home/ubuntu/snorkel-tutorials/.tox/spam/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy when training on original dataset: 0.92\n",
      "Test Accuracy when training on augmented dataset: 0.904\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(train_set, train_labels, num_buckets=30000):\n",
    "    def map_pad_or_truncate(string, max_length=30):\n",
    "        \"\"\"Tokenize text, pad or truncate to get max_length, and hash tokens.\"\"\"\n",
    "        ids = tf.keras.preprocessing.text.hashing_trick(\n",
    "            string, n=num_buckets, hash_function=\"md5\"\n",
    "        )\n",
    "        return ids[:max_length] + [0] * (max_length - len(ids))\n",
    "\n",
    "    # Tokenize training text and convert words to integers.\n",
    "    train_tokens = np.array(list(map(map_pad_or_truncate, train_set.text)))\n",
    "    lstm_model = get_lstm_model(num_buckets)\n",
    "\n",
    "    # Tokenize validation set text and convert words to integers.\n",
    "    valid_tokens = np.array(list(map(map_pad_or_truncate, df_valid.text)))\n",
    "\n",
    "    lstm_model.fit(\n",
    "        train_tokens,\n",
    "        train_labels,\n",
    "        epochs=50,\n",
    "        validation_data=(valid_tokens, Y_valid),\n",
    "        # Set up early stopping based on val set accuracy.\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, verbose=1, restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Tokenize validation set text and convert words to integers.\n",
    "    test_tokens = np.array(list(map(map_pad_or_truncate, df_test.text)))\n",
    "    test_probs = lstm_model.predict(test_tokens)\n",
    "    test_preds = test_probs[:, 0] > 0.5\n",
    "    return (test_preds == Y_test).mean()\n",
    "\n",
    "\n",
    "test_accuracy_original = train_and_test(df_train, Y_train)\n",
    "test_accuracy_augmented = train_and_test(df_train_augmented, Y_train_augmented)\n",
    "\n",
    "print(f\"Test Accuracy when training on original dataset: {test_accuracy_original}\")\n",
    "print(f\"Test Accuracy when training on augmented dataset: {test_accuracy_augmented}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
