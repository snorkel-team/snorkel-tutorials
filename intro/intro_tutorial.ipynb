{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quick walkthrough, we'll preview the high level workflow and interfaces of Snorkel.  For the more detailed version, see the [Introductory Tutorial](#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically Building and Managing Training Data with Snorkel\n",
    "\n",
    "Snorkel is a system for programmatically building and managing training datasets. In Snorkel, users can develop training datasets in hours or days rather than hand-labeling them over weeks or months.\n",
    "\n",
    "Snorkel currently exposes three key programmatic operations: **labeling data**, for example using heuristic rules or distant supervision techniques; **transforming data**, for example rotating or stretching images to perform data augmentation; and **slicing data** into different critical subsets. Snorkel then automatically models, cleans, and integrates the resulting training data using novel, theoretically-grounded techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_ops.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, we'll look at a canonical machine learning problem: classifying spam.  We'll use a public [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle; for more details on this dataset see the [Introductory Tutorial](#).\n",
    "\n",
    "We'll walk through five basic steps:\n",
    "\n",
    "1. **Writing Labeling Functions (LFs):** First, rather than hand-labeling any training data, we'll programamtically label our _unlabeled_ dataset with LFs\n",
    "2. **Modeling & Combining LFs:** Next, we'll use the `LabelModel` to automatically learn the accuracies of our LFs and reweight and combine their outputs\n",
    "3. **Writing Transformation Functions (TFs) for Data Augmentation:** Then, we'll augment this labeled training set by writing a simple TF\n",
    "4. **Writing _Slicing Functions (SFs)_ for Data Subset Selection:** Then, we'll write an SF to identify a critical subset or _slice_ of our training set.\n",
    "5. **Training a final ML model:** Finally, we'll train a simple ML model with our training set!\n",
    "\n",
    "We'll start first by loading the _unlabeled_ comments, which we'll use as our training data, as a pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_unlabeled_spam_dataset\n",
    "df_train = load_unlabeled_spam_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is one comment consisting of text, author, and date values, as well as an integer id for which YouTube video the comment corresponds to.\n",
    "Additionally, since we are looking at the development set, these examples have labels as well- `1` for spam, `0` for ham (not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Writing Labeling Functions\n",
    "\n",
    "_Labeling functions (LFs)_ are one of the core operators for building and managing training datasets programmatically in Snorkel.\n",
    "The basic idea is simple: **a labeling function is a function that labels some subset of the training dataset**.\n",
    "That is, each labeling function either outputs `SPAM`, `HAM`, or `ABSTAIN`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can be used to represent many heuristic strategies for labeling data.\n",
    "**The key idea is that labeling functions do not need to be perfectly accurate**, as Snorkel will automatically estimate their accuracies and correlations, and then reweight and combine their output labels, leading to high-quality training labels.\n",
    "\n",
    "As a starting example, labeling functions can be based on **matching keywords**, using **regular expressions**, leveraging arbitrary **heuristics**, using **third-party models**, and much more- anything that can be expressed as a function that labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob,\n",
    "    combined with the heuristic that ham comments are often positive.\n",
    "    \"\"\"\n",
    "    sentiment = TextBlob(x.text).sentiment\n",
    "    return HAM if sentiment.polarity > 0.3 else ABSTAIN\n",
    "\n",
    "lfs = [\n",
    "    lf_keyword_my,\n",
    "    lf_regex_check_out,\n",
    "    lf_short_comment,\n",
    "    lf_textblob_polarity\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many more types of labeling functions---including over data modalities beyond text---see the other [tutorials](https://github.com/snorkel-team/snorkel-tutorials) and examples at [snorkel.org](http://snorkel.org).\n",
    "In general the process of developing labeling functions is, like any other development process, an iterative one that takes time- but that, in many cases, can be orders-of-magnitude faster that hand-labeling training data.\n",
    "For more detail on the process of developing labeling functions and other training data operators in Snorkel, see the [full version of this tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Combining & Cleaning the Labels\n",
    "\n",
    "Our next step is to apply the labeling functions we wrote to the unlabeled training data; we do this using the `LFApplier` corresponding to our base data class (in this case, the `PandasLFApplier`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1956/1956 [00:00<00:00, 3311.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the labeling functions (LFs) to the data is (for each split of the data) a _label matrix_ with rows corresponding to data points, and columns corresponding to LFs.\n",
    "We can take a look at some statistics of the LFs on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train['label'] = label_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, we have applied the `LabelModel` to the test set, and see that we get an accuracy score of approximately $85\\%$.\n",
    "In many Snorkel applications, it is not possible to apply the labeling functions (and therefore the `LabelModel`) at test time, for example due to the labeling functions using features not available at test time, or being overly slow to execute (for an example of this, see the [cross-modal tutorial](), and other examples at [snorkel.org]()).\n",
    "\n",
    "Here, we _can_ apply the `LabelModel` to the test set, but it leaves a lot to be desired- in part because the labeling functions leave a large portion of the dataset unlabeled:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the final step of the pipeline comes in handy- we will now use the probabilistic training labels to train a machine learning model which will generalize beyond---and outperform---the labeling functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] STEP 3: Writing Transformation Functions for Data Augmentation\n",
    "\n",
    "TODO: Intro text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ajratner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import transformation_function\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Helper function to get the synonyms of word from Wordnet.\"\"\"\n",
    "    lemmas = set().union(*[s.lemmas() for s in wn.synsets(word)])\n",
    "    return list(set([l.name().lower().replace(\"_\", \" \") for l in lemmas]) - {word})\n",
    "\n",
    "@transformation_function()\n",
    "def tf_replace_word_with_synonym(x):\n",
    "    \"\"\"Try to replace a random word with a synonym.\"\"\"\n",
    "    words = x.text.lower().split()\n",
    "    idx = random.choice(range(len(words)))\n",
    "    synonyms = get_synonyms(words[idx])\n",
    "    if len(synonyms) > 0:\n",
    "        x.text = \" \".join(words[:idx] + [synonyms[0]] + words[idx + 1:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply these to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1956/1956 [00:03<00:00, 547.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier \n",
    "\n",
    "tf_policy = ApplyOnePolicy(n_per_original=2, keep_original=True)\n",
    "tf_applier = PandasTFApplier([tf_replace_word_with_synonym], tf_policy)\n",
    "df_train_augmented = tf_applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Training a Machine Learning Model\n",
    "\n",
    "In this final step, our goal is to train a machine learning model that generalizes beyond what the labeling functions label, and thereby outperforms the `LabelModel` above.\n",
    "In this example, **we use an extremely simple ML model**, but still see this generalization effect occur!\n",
    "\n",
    "Note that because the output of the Snorkel `LabelModel` is just a set of labels, Snorkel easily integrates with most popular libraries for performing supervised learning: TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost, etc.\n",
    "\n",
    "In this tutorial we demonstrate using classifiers from Keras and Scikit-Learn. For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text, which we compute using a basic Scikit-Learn `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/snorkel-tutorials-intro/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_train = [row.text for i, row in df_train_augmented.iterrows()]\n",
    "X_train = CountVectorizer(ngram_range=(1, 2)).fit_transform(text_train)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X=X_train, y=df_train_augmented.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe an additional boost in accuracy over the `LabelModel` by multiple points!\n",
    "By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model,\n",
    "we were able to generalize beyond the noisy labeling heuristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrated the basic pipeline of Snorkel, and showed how it can enable us to train high-quality ML models without hand-labeling large training datasets.\n",
    "\n",
    "**Next, check out the extended version of this tutorial---the [spam tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)---which goes into much more detail about the actual process of iterating on labeling functions and other types of operators to build end-to-end ML applications in Snorkel!**\n",
    "\n",
    "You can also check out the [Snorkel 101 Guide](#) and the [`snorkel-tutorials` table of contents](https://github.com/snorkel-team/snorkel-tutorials#snorkel-tutorials) for other tutorials that you may find interesting, including demonstrations of how to use Snorkel:\n",
    "\n",
    "* As part of a [hybrid crowdsourcing pipeline](https://github.com/snorkel-team/snorkel-tutorials/tree/master/crowdsourcing)\n",
    "* For [scene-graph detection over images](https://github.com/snorkel-team/snorkel-tutorials/tree/master/scene_graph)\n",
    "* For [information extraction over text](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spouse)\n",
    "* For [data augmentation](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)\n",
    "\n",
    "and many more!\n",
    "You can also visit the [Snorkel homepage](http://snorkel.org) or [Snorkel API documentation](https://snorkel.readthedocs.io) for more info!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
