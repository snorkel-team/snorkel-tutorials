{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-md"
    ]
   },
   "source": [
    "# Getting Started with Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-md"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"intro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically Building & Managing Training Data with Snorkel\n",
    "\n",
    "Snorkel is a system for _programmatically_ building and managing training datasets **without needing to hand-label _any_ training data**.\n",
    "In Snorkel, users can develop training datasets in hours or days rather than hand-labeling them over weeks or months.\n",
    "\n",
    "Snorkel currently exposes three key programmatic operations:\n",
    "- **Labeling data**, for example using heuristic rules or distant supervision techniques\n",
    "- **Transforming data**, for example rotating or stretching images to perform data augmentation\n",
    "- **Slicing data** into different critical subsets.\n",
    "Snorkel then automatically models, cleans, and integrates the resulting training data using novel, theoretically-grounded techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_ops.png\" onerror=\"this.onerror=null; this.src='/doks-theme/assets/images/layout/TrainingData.png';\" align=\"center\" style=\"display: block; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quick walkthrough, we'll preview the high level workflow and interfaces of Snorkel using a canonical machine learning problem: classifying spam.\n",
    "We'll use a public [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle, and see how **Snorkel can enable you to train a machine learning model without _any_ hand-labeled training data!**\n",
    "For the more detailed version, see the [Introductory Tutorial](#).\n",
    "\n",
    "We'll walk through five basic steps:\n",
    "\n",
    "1. **Writing Labeling Functions (LFs):** First, rather than hand-labeling any training data, we'll programatically label our _unlabeled_ dataset with LFs.\n",
    "2. **Modeling & Combining LFs:** Next, we'll use Snorkel's `LabelModel` to automatically learn the accuracies of our LFs and reweight and combine their outputs into a single, confidence-weighted training label per data point.\n",
    "3. **Writing Transformation Functions (TFs) for Data Augmentation:** Then, we'll augment this labeled training set by writing a simple TF.\n",
    "4. **Writing _Slicing Functions (SFs)_ for Data Subset Selection:** We'll also preview writing an SF to identify a critical subset or _slice_ of our training set.\n",
    "5. **Training a final ML model:** Finally, we'll train a simple ML model with our training set!\n",
    "\n",
    "We'll start first by loading the _unlabeled_ comments, which we'll use as our training data, as a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_unlabeled_spam_dataset\n",
    "\n",
    "df_train = load_unlabeled_spam_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Writing Labeling Functions\n",
    "\n",
    "_Labeling functions (LFs)_ are one of the core operators for building and managing training datasets programmatically in Snorkel.\n",
    "The basic idea is simple: **a labeling function is a function that outputs a label for some subset of the training dataset**.\n",
    "That is, in our example here, each labeling function takes as input a comment data point, and either outputs a label (`SPAM = 1` or `NOT_SPAM = 0`) or abstains from labeling (`ABSTAIN = -1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "NOT_SPAM = 0\n",
    "SPAM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can be used to represent many heuristic and/or noisy strategies for labeling data, often referred to as [weak supervision](#).\n",
    "The basic idea of labeling functions, and other programmatic operators in Snorkel, is to let users inject domain information into machine learning models in higher level, higher bandwidth ways than manually labeling thousands or millions of individual data points.\n",
    "**The key idea is that labeling functions do not need to be perfectly accurate**, and can in fact even be  correlated with each other.\n",
    "Snorkel will automatically estimate their accuracies and correlations in a [provably consistent way](https://papers.nips.cc/paper/6523-data-programming-creating-large-training-sets-quickly), and then reweight and combine their output labels, leading to high-quality training labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our text data setting here, labeling functions can be keyword matchers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flexibly leverage arbitrary heuristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Non-spam comments are often short, such as 'cool video!'.\"\"\"\n",
    "    return NOT_SPAM if len(x.text.split()) < 5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use third-party models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob.\n",
    "\n",
    "    We combine this with the heuristic that non-spam comments are often positive.\n",
    "    \"\"\"\n",
    "    sentiment = TextBlob(x.text).sentiment\n",
    "    return NOT_SPAM if sentiment.polarity > 0.3 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And much more!\n",
    "For many more types of labeling functions–including over data modalities beyond text–see the other [tutorials](#) and [real-world examples](#).\n",
    "\n",
    "In general the process of developing labeling functions is, like any other development process, an iterative one that takes time.\n",
    "However, in many cases it can be _orders-of-magnitude_ faster that hand-labeling training data.\n",
    "For more detail on the process of developing labeling functions and other training data operators in Snorkel, see the [Introductory tutorial](#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Combining & Cleaning the Labels\n",
    "\n",
    "Our next step is to apply the labeling functions we wrote to the unlabeled training data.\n",
    "The result is a _label matrix_, `L_train`, where each row corresponds to a data point and each column corresponds to a labeling function.\n",
    "Since the labeling functions have unknown accuracies and correlations, their output labels may overlap and conflict.\n",
    "We use the `LabelModel` to automatically estimate their accuracies and correlations, reweight and combine their labels, and produce our final set of clean, integrated training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1956/1956 [00:00<00:00, 2900.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LabelModel, PandasLFApplier\n",
    "\n",
    "# Define the set of labeling functions (LFs)\n",
    "lfs = [lf_keyword_my, lf_regex_check_out, lf_short_comment, lf_textblob_polarity]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "\n",
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the `LabelModel` to label data; however, on many data points, all the labeling functions abstain, and so the `LabelModel` abstains as well.\n",
    "We'll filter these examples out of our training set now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.label != ABSTAIN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to use the resulting labeled training data points to train a machine learning model that can **generalize beyond the coverage of the labeling functions and the `LabelModel`**.\n",
    "However first we'll explore some of Snorkel's other operators for building and managing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3): Writing Transformation Functions for Data Augmentation\n",
    "\n",
    "An increasingly popular and critical technique in modern machine learning is [data augmentation](#), the strategy of artificially _augmenting_ existing labeled training datasets by creating transformed copies of the data points.\n",
    "Data augmentation is a practical and powerful method for injecting information about domain invariances into ML models via the data, rather than by trying to modify their internal architectures.\n",
    "The canonical example is randomly rotating, stretching, and transforming images when training image classifiers---a ubiquitous technique in the field of computer vision today.\n",
    "However, data augmentation is increasingly used in a range of settings, including text.\n",
    "\n",
    "Here, we implement a simple text data augmentation strategy- randomly replacing a word with a synonym.\n",
    "We express this as a *transformation function (TF)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get the synonyms of word from Wordnet.\"\"\"\n",
    "    lemmas = set().union(*[s.lemmas() for s in wn.synsets(word)])\n",
    "    return list(set([l.name().lower().replace(\"_\", \" \") for l in lemmas]) - {word})\n",
    "\n",
    "\n",
    "@transformation_function()\n",
    "def tf_replace_word_with_synonym(x):\n",
    "    \"\"\"Try to replace a random word with a synonym.\"\"\"\n",
    "    words = x.text.lower().split()\n",
    "    idx = random.choice(range(len(words)))\n",
    "    synonyms = get_synonyms(words[idx])\n",
    "    if len(synonyms) > 0:\n",
    "        x.text = \" \".join(words[:idx] + [synonyms[0]] + words[idx + 1 :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply this transformation function to our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1387/1387 [00:02<00:00, 549.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier\n",
    "\n",
    "tf_policy = ApplyOnePolicy(n_per_original=2, keep_original=True)\n",
    "tf_applier = PandasTFApplier([tf_replace_word_with_synonym], tf_policy)\n",
    "df_train_augmented = tf_applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a common challenge with data augmentation is figuring out how to tune and apply different transformation functions to best augment a training set.\n",
    "This is most commonly done as an arduous and ad hoc manual process; however, in Snorkel, various approaches for using automatically learned data augmentation _policies_ are supported.\n",
    "For more detail, see the [data augmentation tutorial](#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Writing a Slicing Function\n",
    "\n",
    "Finally, a third operator in Snorkel, *slicing functions (SFs)*, handles the reality that many dataset have certain subsets or _slices_ that are more important than others.\n",
    "In Snorkel, we can write SFs to (a) monitor specific slices and (b) improve model performance over them by adding representational capacity.\n",
    "\n",
    "Writing a slicing function is simple.\n",
    "For example, we could write one that looks for suspiciously shortened links, which might be critical due to their likelihood of linking to malicious sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "\n",
    "@slicing_function()\n",
    "def short_link(x):\n",
    "    \"\"\"Return whether text matches common pattern for shortened \".ly\" links.\"\"\"\n",
    "    return bool(re.search(r\"\\w+\\.ly\", x.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Snorkel monitor the performance over this slice, and to add representational capacity to whatever model we are using in order to increase performance.\n",
    "For a walkthrough of these steps, see the [slicing tutorial](#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Training a Machine Learning Model\n",
    "\n",
    "The ultimate goal in Snorkel is to **create a training dataset**, which can then be plugged into any machine learning framework (e.g. TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost) to train powerful machine learning models.\n",
    "Here, to complete this initial walkthrough, we'll train an extremely simple model–a simple \"bag of n-grams\" logistic regression model in SciKit-Learn–using the weakly labeled and augmented training set we made with our labeling and transformation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/snorkel-tutorials-intro/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_train = [row.text for i, row in df_train_augmented.iterrows()]\n",
    "X_train = CountVectorizer(ngram_range=(1, 2)).fit_transform(text_train)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X=X_train, y=df_train_augmented.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it- you've trained your first model **without hand-labeling _any_ training data!**\n",
    "Next, to learn more about Snorkel, check out the [tutorials](#), [use cases](#), [resources](#), and [documentation](#) for much more on how to use Snorkel to power your own machine learning applications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
