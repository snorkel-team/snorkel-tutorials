{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Snorkel tutorial, we will walk through the basics of Snorkel, using it to fight YouTube comments spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel Basics\n",
    "\n",
    "**Snorkel is a system for programmatically building and managing training datasets to rapidly and flexibly fuel machine learning models.**\n",
    "\n",
    "Today's state-of-the-art machine learning models are more powerful and easy to use than ever before- however, they require massive _training datasets_.\n",
    "For example, if we wanted to use one of the latest and greatest machine learning models to classify YouTube comments as spam or not, we'd need to first hand-label a large number of YouTube comments---a *training set*---that our model would learn from.\n",
    "\n",
    "Building and managing training datasets often requires slow and prohibitively expensive manual effort by domain experts (especially when data is private or requires expensive expert labelers).\n",
    "In Snorkel, users instead write **programmatic operations to label, transform, and structure training datasets** for machine learning, without needing to hand label any training data; Snorkel then uses novel, theoretically-grounded modeling techniques to clean and integrate the resulting training data.\n",
    "In a wide range of applications---from medical image monitoring to text information extraction to industrial deployments over web data---Snorkel provides a radically faster and more flexible to build machine learning applications; see [snorkel.org](snorkel.org) for more detail on many examples of Snorkel usage!\n",
    "\n",
    "In this intro tutorial, we'll see how Snorkel can let us train a machine learning model for spam classification _without_ hand-labeling anything but a small test and validation set (i.e., without hand-labeling _any_ training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Snorkel Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_101_pipeline.png\" align=\"left\">`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snorkel is a system for programmatically building and managing training datasets in a number of ways- we'll start with **labeling** training data.\n",
    "Here, the basic pipeline consists of three main steps:\n",
    "\n",
    "1. **Writing Labeling Functions:** First, instead of labeling the training data by hand, we will write _labeling functions_, special Python functions that label subsets of the training data heuristically.\n",
    "\n",
    "2. **Combining & cleaning the labels:** The labeling functions we write will have varying accuracies, coverages, and correlations- leading to complex overlaps and disagreements. We will use Snorkel's `LabelModel` to automatically reweight and combine the outputs of the labeling functions, resulting in clean, _probabilistic_ training labels.\n",
    "\n",
    "3. **Training a machine learning model:** Finally, we'll show how to use the probabilistic training labels from step (2) to train a machine learning model, which we'll show will generalize beyond and outperform the labeling functions!\n",
    "\n",
    "For much more on Snorkel---including four years of academic papers, applications, and more!---see [snorkel.org](http://snorkel.org).\n",
    "You can also check out the [Snorkel API documentation](https://snorkel.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: Classifying YouTube Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_101_spam.png\" width=\"500px\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll focus on a [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle that consists of YouTube comments from 5 videos.\n",
    "**For a much more detailed version of this tutorial, see the Snorkel [spam tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam).**\n",
    "\n",
    "The simple classification task we focus on here is a classic one in the history of machine learning- classifying each comment as being \"spam\" or \"ham\" (not spam); more specifically, we aim to train a classifier that outputs one of the following labels for each YouTube comment:\n",
    "\n",
    "* **`SPAM`**: irrelevant or inappropriate messages, or\n",
    "* **`HAM`**: comments relevant to the video\n",
    "\n",
    "For example, the following comments are `SPAM`:\n",
    "\n",
    "        \"Subscribe to me for free Android games, apps..\"\n",
    "\n",
    "        \"Please check out my vidios\"\n",
    "\n",
    "        \"Subscribe to me and I'll subscribe back!!!\"\n",
    "\n",
    "and these are `HAM`:\n",
    "\n",
    "        \"3:46 so cute!\"\n",
    "\n",
    "        \"This looks so fun and it's a good song\"\n",
    "\n",
    "        \"This is a weird video.\"\n",
    "        \n",
    "For our task, we have access to a large amount of *unlabeled YouTube comments*, which forms our **training set**.\n",
    "We also have access to a small amount of labeled data, which we split into **development set** (for looking at while developing labeling functions), a **validation set** (for model hyperparameter tuning), and a **test set** (for final evaluation).\n",
    "We load this data in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "# Load data- for details see the spam tutorial\n",
    "df_train, *_ = load_spam_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is one comment consisting of text, author, and date values, as well as an integer id for which YouTube video the comment corresponds to.\n",
    "Additionally, since we are looking at the development set, these examples have labels as well- `1` for spam, `0` for ham (not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Writing Labeling Functions\n",
    "\n",
    "_Labeling functions (LFs)_ are one of the core operators for building and managing training datasets programmatically in Snorkel.\n",
    "The basic idea is simple: **a labeling function is a function that labels some subset of the training dataset**.\n",
    "That is, each labeling function either outputs `SPAM`, `HAM`, or `ABSTAIN`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can be used to represent many heuristic strategies for labeling data.\n",
    "**The key idea is that labeling functions do not need to be perfectly accurate**, as Snorkel will automatically estimate their accuracies and correlations, and then reweight and combine their output labels, leading to high-quality training labels.\n",
    "\n",
    "As a starting example, labeling functions can be based on **matching keywords**, using **regular expressions**, leveraging arbitrary **heuristics**, using **third-party models**, and much more- anything that can be expressed as a function that labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf import labeling_function\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob,\n",
    "    combined with the heuristic that ham comments are often positive.\n",
    "    \"\"\"\n",
    "    sentiment = TextBlob(x.text).sentiment\n",
    "    return HAM if sentiment.polarity > 0.3 else ABSTAIN\n",
    "\n",
    "lfs = [\n",
    "    lf_keyword_my,\n",
    "    lf_regex_check_out,\n",
    "    lf_short_comment,\n",
    "    lf_textblob_polarity\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many more types of labeling functions---including over data modalities beyond text---see the other [tutorials](https://github.com/snorkel-team/snorkel-tutorials) and examples at [snorkel.org](http://snorkel.org).\n",
    "In general the process of developing labeling functions is, like any other development process, an iterative one that takes time- but that, in many cases, can be orders-of-magnitude faster that hand-labeling training data.\n",
    "For more detail on the process of developing labeling functions and other training data operators in Snorkel, see the [full version of this tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Combining & Cleaning the Labels\n",
    "\n",
    "Our next step is to apply the labeling functions we wrote to the unlabeled training data; we do this using the `LFApplier` corresponding to our base data class (in this case, the `PandasLFApplier`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:00<00:00, 3370.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.apply.pandas import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the labeling functions (LFs) to the data is (for each split of the data) a _label matrix_ with rows corresponding to data points, and columns corresponding to LFs.\n",
    "We can take a look at some statistics of the LFs on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "Y_preds_train = label_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, we have applied the `LabelModel` to the test set, and see that we get an accuracy score of approximately $85\\%$.\n",
    "In many Snorkel applications, it is not possible to apply the labeling functions (and therefore the `LabelModel`) at test time, for example due to the labeling functions using features not available at test time, or being overly slow to execute (for an example of this, see the [cross-modal tutorial](), and other examples at [snorkel.org]()).\n",
    "\n",
    "Here, we _can_ apply the `LabelModel` to the test set, but it leaves a lot to be desired- in part because the labeling functions leave a large portion of the dataset unlabeled:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the final step of the pipeline comes in handy- we will now use the probabilistic training labels to train a machine learning model which will generalize beyond---and outperform---the labeling functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Training a Machine Learning Model\n",
    "\n",
    "In this final step, our goal is to train a machine learning model that generalizes beyond what the labeling functions label, and thereby outperforms the `LabelModel` above.\n",
    "In this example, **we use an extremely simple ML model**, but still see this generalization effect occur!\n",
    "\n",
    "Note that because the output of the Snorkel `LabelModel` is just a set of labels, Snorkel easily integrates with most popular libraries for performing supervised learning: TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost, etc.\n",
    "\n",
    "In this tutorial we demonstrate using classifiers from Keras and Scikit-Learn. For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text, which we compute using a basic Scikit-Learn `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/snorkel-tutorials-intro/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_train = [row.text for i, row in df_train.iterrows()]\n",
    "X_train = CountVectorizer(ngram_range=(1, 2)).fit_transform(text_train)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X=X_train, y=Y_preds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe an additional boost in accuracy over the `LabelModel` by multiple points!\n",
    "By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model,\n",
    "we were able to generalize beyond the noisy labeling heuristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrated the basic pipeline of Snorkel, and showed how it can enable us to train high-quality ML models without hand-labeling large training datasets.\n",
    "\n",
    "**Next, check out the extended version of this tutorial---the [spam tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)---which goes into much more detail about the actual process of iterating on labeling functions and other types of operators to build end-to-end ML applications in Snorkel!**\n",
    "\n",
    "You can also check out the [Snorkel 101 Guide](#) and the [`snorkel-tutorials` table of contents](https://github.com/snorkel-team/snorkel-tutorials#snorkel-tutorials) for other tutorials that you may find interesting, including demonstrations of how to use Snorkel:\n",
    "\n",
    "* As part of a [hybrid crowdsourcing pipeline](https://github.com/snorkel-team/snorkel-tutorials/tree/master/crowdsourcing)\n",
    "* For [scene-graph detection over images](https://github.com/snorkel-team/snorkel-tutorials/tree/master/scene_graph)\n",
    "* For [information extraction over text](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spouse)\n",
    "* For [data augmentation](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)\n",
    "\n",
    "and many more!\n",
    "You can also visit the [Snorkel homepage](http://snorkel.org) or [Snorkel API documentation](https://snorkel.readthedocs.io) for more info!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
