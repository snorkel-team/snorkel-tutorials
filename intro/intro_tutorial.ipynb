{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Snorkel tutorial, we will walk through the basics of Snorkel, using it to fight YouTube comments spam as our first example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel Basics\n",
    "\n",
    "**Snorkel is a system for programmatically building and managing training datasets to rapidly and flexibly fuel machine learning models.**\n",
    "\n",
    "Today's state-of-the-art machine learning models are more powerful and easy to use than ever before- however, they require massive _training datasets_.\n",
    "For example, if we wanted to use one of the latest and greatest machine learning models to classify YouTube comments as spam or not, we'd need to first hand-label a large number of YouTube comments---a *training set*---that our model would learn from.\n",
    "\n",
    "Building and managing training datasets often requires slow and prohibitively expensive manual effort by domain experts (especially when data is private or requires expensive expert labelers).\n",
    "In Snorkel, users instead write **programmatic operations to label, transform, and structure training datasets** for machine learning, without needing to hand label any training data; Snorkel then uses novel, theoretically-grounded modeling techniques to clean and integrate the resulting training data.\n",
    "In a wide range of applications---from medical image monitoring to text information extraction to industrial deployments over web data---Snorkel provides a radically faster and more flexible to build machine learning applications; see [snorkel.org](snorkel.org) for more detail on the many examples of Snorkel usage!\n",
    "\n",
    "In this intro tutorial, we'll see how Snorkel can let us train a machine learning model for spam classification _without_ hand-labeling anything but a small test and validation set (i.e., without hand-labeling _any_ training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Snorkel Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_101_pipeline.png\" align=\"left\">`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snorkel is a system for programmatically building and managing training datasets in a number of ways- we'll start with **labeling** training data.\n",
    "Here, the basic pipeline consists of three main steps:\n",
    "\n",
    "1. **Writing Labeling Functions:** First, instead of labeling the training data by hand, we will write _labeling functions_, special Python functions that label subsets of the training data heuristically.\n",
    "\n",
    "2. **Combining & cleaning the labels:** The labeling functions we write will have varying accuracies, coverages, and correlations- leading to complex overlaps and disagreements. We will use Snorkel's `LabelModel` to automatically reweight and combine the outputs of the labeling functions, resulting in clean, _probabilistic_ training labels.\n",
    "\n",
    "3. **Training a machine learning model:** Finally, we'll show how to use the probabilistic training labels from step (2) to train a machine learning model, which we'll show will generalize beyond and outperform the labeling functions!\n",
    "\n",
    "For much more on Snorkel---including four years of academic papers, applications, and more!---see [snorkel.org](http://snorkel.org).\n",
    "You can also check out the [Snorkel API documentation](https://snorkel.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: Classifying YouTube Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/snorkel_101_spam.png\" width=\"500px\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll focus on a [YouTube comments dataset](https://www.kaggle.com/goneee/youtube-spam-classifiedcomments) from Kaggle that consists of YouTube comments from 5 videos.\n",
    "**For a much more detailed version of this tutorial, see the Snorkel [spam tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam).**\n",
    "\n",
    "The simple classification task we focus on here is a classic one in the history of machine learning- classifying each comment as being \"spam\" or \"ham\" (not spam); more specifically, we aim to train a classifier that outputs one of the following labels for each YouTube comment:\n",
    "\n",
    "* **`SPAM`**: irrelevant or inappropriate messages, or\n",
    "* **`HAM`**: comments relevant to the video\n",
    "\n",
    "For example, the following comments are `SPAM`:\n",
    "\n",
    "        \"Subscribe to me for free Android games, apps..\"\n",
    "\n",
    "        \"Please check out my vidios\"\n",
    "\n",
    "        \"Subscribe to me and I'll subscribe back!!!\"\n",
    "\n",
    "and these are `HAM`:\n",
    "\n",
    "        \"3:46 so cute!\"\n",
    "\n",
    "        \"This looks so fun and it's a good song\"\n",
    "\n",
    "        \"This is a weird video.\"\n",
    "        \n",
    "For our task, we have access to a large amount of *unlabeled YouTube comments*, which forms our **training set**.\n",
    "We also have access to a small amount of labeled data, which we split into **development set** (for looking at while developing labeling functions), a **validation set** (for model hyperparameter tuning), and a **test set** (for final evaluation).\n",
    "We load this data in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure we're running from the spam/ directory\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "# Load data- for details see the spam tutorial\n",
    "df_train, df_dev, df_valid, df_test = load_spam_dataset()\n",
    "Y_dev = df_dev[\"label\"].values\n",
    "Y_valid = df_valid[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at a few data points, which have been loaded in as Pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Pepe The Meme King</td>\n",
       "      <td>2015-05-19T03:49:29.427000</td>\n",
       "      <td>everyday I&amp;#39;m shufflin﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Melissa Erhart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Angel</td>\n",
       "      <td>2014-11-02T17:27:09</td>\n",
       "      <td>Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sandeep Singh</td>\n",
       "      <td>2015-05-23T17:51:58.957000</td>\n",
       "      <td>Charlie from LOST﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>BigBird Larry</td>\n",
       "      <td>2015-05-24T09:48:00.835000</td>\n",
       "      <td>Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                        date  \\\n",
       "128  Pepe The Meme King  2015-05-19T03:49:29.427000   \n",
       "151  Melissa Erhart      NaN                          \n",
       "31   Angel               2014-11-02T17:27:09          \n",
       "29   Sandeep Singh       2015-05-23T17:51:58.957000   \n",
       "237  BigBird Larry       2015-05-24T09:48:00.835000   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "128  everyday I&#39;m shufflin﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "151  Check out this playlist on YouTube:chcfcvzfzfbvzdr﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "31   Hi there~I'm group leader of Angel, a rookie Korean pop group. We have four  members, Chanicka, Julie, Stephanie, and myself, Leah. Please feel free to  check out our channel and leave some feedback on our cover videos (:  criticism is welcome as we know we're not top notch singers so please come  leave some constructive feedback on our videos; we appreciate any chance to  improve before auditioning for a Korean management company. We plan on  auditioning for JYP, BigHit, Jellyfish, YG or SM. Thank you for taking time  out of your day to read this !﻿   \n",
       "29   Charlie from LOST﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "237  Every single one of his songs brings me back to place I can never go back to and it hurts so bad inside﻿                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "     label  video  \n",
       "128  0      3      \n",
       "151  1      4      \n",
       "31   1      1      \n",
       "29   0      4      \n",
       "237  0      4      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "df_dev.sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is one comment consisting of text, author, and date values, as well as an integer id for which YouTube video the comment corresponds to.\n",
    "Additionally, since we are looking at the development set, these examples have labels as well- `1` for spam, `0` for ham (not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Writing Labeling Functions\n",
    "\n",
    "_Labeling functions (LFs)_ are one of the core operators for building and managing training datasets programmatically in Snorkel.\n",
    "The basic idea is simple: **a labeling function is a function that labels some subset of the training dataset**.\n",
    "That is, each labeling function either outputs `SPAM`, `HAM`, or `ABSTAIN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM = 0\n",
    "SPAM = 1\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can be used to represent many heuristic strategies for labeling data.\n",
    "**The key idea is that labeling functions do not need to be perfectly accurate**, as Snorkel will automatically estimate their accuracies and correlations, and then reweight and combine their output labels, leading to high-quality training labels.\n",
    "\n",
    "As a starting example, labeling functions can be based on heuristically **matching keywords**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.lf import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can also use **regular expressions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can of course flexibly express a range of other **heuristics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Ham comments are often short, such as 'cool video!'\"\"\"\n",
    "    return HAM if len(x.text.split()) < 5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions can also use **third-party models**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob,\n",
    "    combined with the heuristic that ham comments are often positive.\n",
    "    \"\"\"\n",
    "    sentiment = TextBlob(x.text).sentiment\n",
    "    return HAM if sentiment.polarity > 0.3 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many more types of labeling functions---including over data modalities beyond text---see the other [tutorials](https://github.com/snorkel-team/snorkel-tutorials) and examples at [snorkel.org](http://snorkel.org).\n",
    "In general the process of developing labeling functions is, like any other development process, an iterative one that takes time- but that, in [many cases](http://snorkel.org), can be orders-of-magnitude faster that hand-labeling training data.\n",
    "For more detail on the process of developing labeling functions and other training data operators in Snorkel, see the [full version of this tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam).\n",
    "\n",
    "For now, we proceed by loading the above labeling functions plus some additional similar ones from a utility file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lfs import lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Combining & Cleaning the Labels\n",
    "\n",
    "Our next step is to apply the labeling functions we wrote to the unlabeled training data; we do this using the `LFApplier` corresponding to our base data class (in this case, the `PandasLFApplier`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:01<00:00, 1228.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1371.35it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 1064.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.apply import PandasLFApplier\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "L_dev = applier.apply(df_dev)\n",
    "L_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the labeling functions (LFs) to the data is (for each split of the data) a sparse _label matrix_ with rows corresponding to data points, and columns corresponding to LFs.\n",
    "We can take a look at some statistics of the LFs on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lf_keyword_my</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_subscribe</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_link</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_please</th>\n",
       "      <td>3</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_song</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_regex_check_out</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.10</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_short_comment</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_textblob_polarity</th>\n",
       "      <td>7</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.11</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_textblob_subjectivity</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "lf_keyword_my             0  [1]      0.22      0.21      0.13       19        \n",
       "lf_subscribe              1  [1]      0.08      0.05      0.02       8         \n",
       "lf_link                   2  [1]      0.10      0.07      0.05       10        \n",
       "lf_please                 3  [1]      0.10      0.09      0.05       10        \n",
       "lf_song                   4  [0]      0.16      0.11      0.06       11        \n",
       "lf_regex_check_out        5  [1]      0.29      0.18      0.10       29        \n",
       "lf_short_comment          6  [0]      0.28      0.13      0.05       19        \n",
       "lf_textblob_polarity      7  [0]      0.28      0.23      0.11       18        \n",
       "lf_textblob_subjectivity  8  [0]      0.08      0.08      0.04       5         \n",
       "\n",
       "                          Incorrect  Emp. Acc.  \n",
       "lf_keyword_my             3          0.863636   \n",
       "lf_subscribe              0          1.000000   \n",
       "lf_link                   0          1.000000   \n",
       "lf_please                 0          1.000000   \n",
       "lf_song                   5          0.687500   \n",
       "lf_regex_check_out        0          1.000000   \n",
       "lf_short_comment          9          0.678571   \n",
       "lf_textblob_polarity      10         0.642857   \n",
       "lf_textblob_subjectivity  3          0.625000   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling.analysis import LFAnalysis\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the LFs indeed have varying accuracies, conflict with each other on non-negligible portions of the training set, and also each only cover some subset of the training set.\n",
    "Next, we use Snorkel's `LabelModel` to automatically learn the accuracies* and correlations of the LFs, and reweight and combine their outputs into a final set of _probabilistic_ training labels.\n",
    "\n",
    "*_Note that the \"Empirical Accuracies\" in the table above are computed over a very small labeled development set that we looked at while writing the LFs- thus, it is a very biased estimate of their accuracies (this is, for example, why so many of them have 100% accuracy according to this estimate...).  Thus, we rely on a [novel unsupervised generative modeling technique](https://arxiv.org/abs/1605.07723) in Snorkel to learn their accuracies in a provably convergent way over the much larger unlabeled training set. For more technical details of this overall approach, see our [NeurIPS 2016](https://arxiv.org/abs/1605.07723) and [AAAI 2019](https://arxiv.org/abs/1810.02840) papers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[0 epochs]: TRAIN:[loss=0.127]\n",
      "[50 epochs]: TRAIN:[loss=0.012]\n",
      "[100 epochs]: TRAIN:[loss=0.010]\n",
      "[150 epochs]: TRAIN:[loss=0.009]\n",
      "[200 epochs]: TRAIN:[loss=0.008]\n",
      "[250 epochs]: TRAIN:[loss=0.008]\n",
      "[300 epochs]: TRAIN:[loss=0.008]\n",
      "[350 epochs]: TRAIN:[loss=0.008]\n",
      "[400 epochs]: TRAIN:[loss=0.008]\n",
      "[450 epochs]: TRAIN:[loss=0.008]\n",
      "Finished Training\n",
      "Label Model Accuracy:     84.8%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "# Training the LabelModel\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "\n",
    "# Getting the probabilistic labels for the training set from the LabelModel\n",
    "Y_probs_train = label_model.predict_proba(L=L_train)\n",
    "\n",
    "# Computing the accuracy of the LabelModel\n",
    "label_model_acc = label_model.score(L_test, Y_test)[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, we have applied the `LabelModel` to the test set, and see that we get an accuracy score of approximately $85\\%$.\n",
    "In many Snorkel applications, it is not possible to apply the labeling functions (and therefore the `LabelModel`) at test time, for example due to the labeling functions using features not available at test time, or being overly slow to execute (for an example of this, see the [cross-modal tutorial](), and other examples at [snorkel.org]()).\n",
    "\n",
    "Here, we _can_ apply the `LabelModel` to the test set, but it leaves a lot to be desired- in part because the labeling functions leave a large portion of the dataset unlabeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import plot_label_frequency\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, where is where the final step of the pipeline comes in handy- we will now use the probabilistic training labels to train a machine learning model which will generalize beyond---and outperform---the labeling functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Training a Machine Learning Model\n",
    "\n",
    "In this final step, our goal is to train a machine learning model that generalizes beyond what the labeling functions label, and thereby outperforms the `LabelModel` above.\n",
    "In this example, **we use an extremely simple ML model**, but still see this generalization effect occur!\n",
    "\n",
    "Note that because the output of the Snorkel `LabelModel` is just a set of labels, Snorkel easily integrates with most popular libraries for performing supervised learning: TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, XGBoost, etc.\n",
    "\n",
    "In this tutorial we demonstrate using classifiers from Keras and Scikit-Learn. For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text, which we compute using a basic Scikit-Learn `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform([row.text for i, row in df_train.iterrows()])\n",
    "X_valid = vectorizer.transform([row.text for i, row in df_valid.iterrows()])\n",
    "X_test = vectorizer.transform([row.text for i, row in df_test.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: This step will be removed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = L_train.sum(axis=1) != ABSTAIN * len(lfs)\n",
    "X_train = X_train[mask, :]\n",
    "Y_probs_train = Y_probs_train[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use Keras, a popular high-level API for building models in TensorFlow, to build a simple logistic regression classifier.\n",
    "We compile it with a `categorical_crossentropy` loss so that it can handle probabilistic labels instead of integer labels.\n",
    "Using a _noise-aware loss_ &mdash; one that uses probabilistic labels &mdash; for our discriminative model lets\n",
    "us take full advantage of the label model's learning procedure (see our [NeurIPS 2016 paper](https://arxiv.org/abs/1605.07723)).\n",
    "We use the common settings of an `Adam` optimizer and early stopping (evaluating the model on the validation set after each epoch and reloading the weights from when it achieved the best score).\n",
    "For more information on Keras, see the [Keras documentation](https://keras.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 20:31:38.595427 4453356992 deprecation.py:506] From /Users/ajratner/repos/snorkel-tutorials/.env_intro/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "Test Accuracy: 0.924\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis.utils import probs_to_preds, preds_to_probs\n",
    "from snorkel.analysis.metrics import metric_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Our model is a simple linear layer mapping from feature\n",
    "# vectors to the number of labels in our problem (2).\n",
    "keras_model = tf.keras.Sequential()\n",
    "keras_model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        2,\n",
    "        input_dim=X_train.shape[1],\n",
    "        activation=tf.nn.softmax,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "keras_model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\", patience=10, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "keras_model.fit(\n",
    "    X_train,\n",
    "    Y_probs_train,\n",
    "    validation_data=(X_valid, preds_to_probs(Y_valid, 2)),\n",
    "    callbacks=[early_stopping],\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "Y_preds_test = probs_to_preds(keras_model.predict(X_test))\n",
    "print(f\"Test Accuracy: {metric_score(Y_test, Y_preds_test, metric='accuracy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe an additional boost in accuracy over the `LabelModel` by multiple points!\n",
    "By using the label model to transfer the domain knowledge encoded in our LFs to the discriminative model,\n",
    "we were able to generalize beyond the noisy labeling heuristics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrated the basic pipeline of Snorkel, and showed how it can enable us to train high-quality ML models without hand-labeling large training datasets.\n",
    "\n",
    "**Next, check out the extended version of this tutorial---the [spam tutorial](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)---which goes into much more detail about the actual process of iterating on labeling functions and other types of operators to build end-to-end ML applications in Snorkel!**\n",
    "\n",
    "You can also check out the [Snorkel 101 Guide](#) and the [`snorkel-tutorials` table of contents](https://github.com/snorkel-team/snorkel-tutorials#snorkel-tutorials) for other tutorials that you may find interesting, including demonstrations of how to use Snorkel:\n",
    "\n",
    "* As part of a [hybrid crowdsourcing pipeline](https://github.com/snorkel-team/snorkel-tutorials/tree/master/crowdsourcing)\n",
    "* For [scene-graph detection over images](https://github.com/snorkel-team/snorkel-tutorials/tree/master/scene_graph)\n",
    "* For [information extraction over text](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spouse)\n",
    "* For [data augmentation](https://github.com/snorkel-team/snorkel-tutorials/tree/master/spam)\n",
    "\n",
    "and many more!\n",
    "You can also visit the [Snorkel homepage](http://snorkel.org) or [Snorkel API documentation](https://snorkel.readthedocs.io) for more info!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
